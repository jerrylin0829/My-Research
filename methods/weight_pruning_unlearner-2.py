"""
WeightPruning éºå¿˜å™¨ - æ™ºèƒ½æ¬Šé‡ä¿®å‰ªç­–ç•¥
"""
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.nn.utils.prune as prune
import numpy as np
from tqdm import tqdm
import math
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.base_unlearner import BaseUnlearner


class WeightPruningUnlearner(BaseUnlearner):
    """
    æ¬Šé‡ä¿®å‰ªéºå¿˜ï¼šç§»é™¤èˆ‡éºå¿˜æ•¸æ“šç›¸é—œçš„æ¬Šé‡
    
    ç”¨é€”èˆ‡æ„ç¾©ï¼š
    1. ç›´æ¥ç§»é™¤èˆ‡éºå¿˜æ•¸æ“šç›¸é—œçš„ç¥ç¶“é€£æ¥
    2. ç ”ç©¶æ¬Šé‡é‡è¦æ€§èˆ‡éºå¿˜çš„é—œä¿‚
    3. å¯¦ç¾æ°¸ä¹…æ€§çš„çŸ¥è­˜ç§»é™¤
    """
    
    def unlearn(self, 
                retain_loader, 
                forget_loader, 
                prune_ratio=0.1, 
                prune_strategy='magnitude_reset', 
                target_layers=['head', 'late_blocks'],
                fine_tune_epochs=20,
                prune_mode='reset',
                **ft_kwargs
    ):
        """
        æ¬Šé‡ä¿®å‰ªéºå¿˜ä¸»æ–¹æ³•
        
        Args:
            retain_loader: ä¿ç•™æ•¸æ“šåŠ è¼‰å™¨
            forget_loader: éºå¿˜æ•¸æ“šåŠ è¼‰å™¨
            prune_ratio: ä¿®å‰ªæ¯”ä¾‹
            prune_strategy: ä¿®å‰ªç­–ç•¥ ('magnitude', 'gradient', 'fisher')
            target_layers: ç›®æ¨™å±¤
            fine_tune_epochs: ä¿®å‰ªå¾Œå¾®èª¿è¼ªæ•¸
        """
        
        print(f"é–‹å§‹æ¬Šé‡ä¿®å‰ªéºå¿˜ï¼Œç­–ç•¥: {prune_strategy}, ä¿®å‰ªæ¯”ä¾‹: {prune_ratio}")
        print("ä¿æŒåŸå§‹ 100 é¡åˆ†é¡é ­ï¼Œä¸é€²è¡Œé¡åˆ¥æ˜ å°„")
        
        # çµ¦ L2-SPï¼ˆç›®å‰ä¸å•Ÿç”¨ï¼‰é ç•™çš„åŸå§‹æ‹·è²
        self._orig_sd = {k: v.detach().clone().cpu() for k, v in self.model.state_dict().items()}

        if prune_strategy == 'magnitude_zero_lock':
            # self._magnitude_based_pruning(prune_ratio, target_layers, add_noise = False)
            # if prune_mode == 'zero_lock':
            self._magnitude_prune_zero_lock(prune_ratio, target_layers)
        elif prune_strategy == 'magnitude_reset':
            self._magnitude_prune_reset(prune_ratio, target_layers)
        elif prune_strategy == 'gradient':
            self._gradient_based_pruning(retain_loader, forget_loader, prune_ratio, target_layers)
        elif prune_strategy == 'fisher':
            self._fisher_based_pruning(retain_loader, prune_ratio, target_layers)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ä¿®å‰ªç­–ç•¥: {prune_strategy}")
        
        # ä¿®å‰ªå¾Œå¾®èª¿
        if retain_loader is not None and fine_tune_epochs > 0:
            self._fine_tune_after_pruning(retain_loader, fine_tune_epochs, **ft_kwargs)
        
        return self.model
    
    def _magnitude_based_pruning(self, 
                                 prune_ratio, 
                                 target_layers, 
                                 add_noise=False, 
                                 noise_std=1e-2):
        """
        åŸºæ–¼æ¬Šé‡å¤§å°çš„ä¿®å‰ª
        æ–°å¢é›œè¨Šè€Œéç›´æ¥ç½®é›¶
        """
        print("åŸ·è¡ŒåŸºæ–¼æ¬Šé‡å¤§å°çš„ä¿®å‰ª...")
        pruned_count = 0
        total_params = 0
        
        for name, module in self.model.named_modules():
            if self._should_prune_layer(name, target_layers) and hasattr(module, 'weight'):
                weight = module.weight.data
                total_params += weight.numel()
                
                # è¨ˆç®—æ¬Šé‡çš„é‡è¦æ€§åˆ†æ•¸ï¼ˆçµ•å°å€¼ï¼‰
                importance = torch.abs(weight)
                
                # è¨ˆç®—é–¾å€¼ï¼ˆä¿ç•™æœ€é‡è¦çš„æ¬Šé‡ï¼‰
                threshold = torch.quantile(importance.flatten(), prune_ratio)
                
                # å‰µå»ºæ©ç¢¼ï¼ˆå°æ–¼é–¾å€¼çš„æ¬Šé‡è¢«ä¿®å‰ªï¼‰
                mask = importance >= threshold
                pruning_mask = importance < threshold

                if add_noise:
                    # ğŸ› ï¸ æ·»åŠ å°é‡éš¨æ©Ÿå™ªè²è€Œéç½®é›¶
                    noise_scale = weight[mask].std() * noise_std
                    noise = torch.randn_like(weight[pruning_mask]) * noise_scale
                    module.weight.data[pruning_mask] = noise
                else:
                    module.weight.data *= mask.float()
                
                with torch.no_grad():
                    if weight.dim() >= 2:
                        # 2Dæˆ–æ›´é«˜ç¶­åº¦ï¼šä½¿ç”¨fan_in
                        fan_in = weight.size(-1)  # æœ€å¾Œä¸€ç¶­ä½œç‚ºfan_in
                        std = (2.0 / fan_in) ** 0.5
                    else:
                        # 1Dæ¬Šé‡ï¼šä½¿ç”¨ç•¶å‰æ¬Šé‡çš„æ¨™æº–å·®
                        std = weight.std().item() if weight.numel() > 1 else 0.01
                    
                    # é‡æ–°åˆå§‹åŒ–è¢«ä¿®å‰ªçš„æ¬Šé‡
                    module.weight.data[~mask] = torch.randn_like(
                        module.weight.data[~mask]
                    ) * std * 0.1

                # çµ±è¨ˆä¿®å‰ªæ•¸é‡
                pruned_count += pruning_mask.sum().item()
                
                print(f"ä¿®å‰ªå±¤ {name}: {pruning_mask.sum().item()}/{weight.numel()} "
                      f"({100 * pruning_mask.sum().item() / weight.numel():.1f}%)")

        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_count:,}/{total_params:,} ({100*pruned_count/total_params:.1f}%)")
    
    def _magnitude_prune_reset(self, prune_ratio, target_layers):
        """
        reset ç‰ˆï¼šå°æ¬Šé‡ => ç›´æ¥ç½® 0ï¼Œä¸æ› maskï¼Œå¾ŒçºŒå¯è¢«æ¢¯åº¦æ›´æ–°
        """
        print("[Pruning] reset mode (no mask; weights can grow again)")
        pruned_total = 0
        total_params = 0

        for name, module in self.model.named_modules():
            if not self._should_prune_layer(name, target_layers): 
                continue
            if not hasattr(module, 'weight'):
                continue

            W = module.weight.data
            total_params += W.numel()

            imp = torch.abs(W)
            thr = torch.quantile(imp.flatten(), prune_ratio)
            keep_mask = (imp >= thr)
            prune_mask = ~keep_mask

            # 1) ç›´æ¥æŠŠè¢«å‰ªä½ç½®è¨­ç‚º 0ï¼ˆä¸æ›ä»»ä½• maskï¼‰
            W[prune_mask] = 0.0

            pruned_total += prune_mask.sum().item()
            ratio = 100 * prune_mask.float().mean().item()
            print(f"ä¿®å‰ªå±¤ {name}: {int(prune_mask.sum())}/{W.numel()} ({ratio:.1f}%)")

        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_total:,}/{total_params:,} ({100*pruned_total/max(total_params,1):.1f}%)")
        
    def _magnitude_prune_zero_lock(self, prune_ratio, target_layers, noise_std=0.0):
        """
        magnitude æ±ºå®šé®ç½© + torch.prune æ› reparamï¼ˆä¸ removeï¼‰
        âœ forward/ backward æ°¸é ç”¨ weight_orig * maskï¼Œè¢«å‰ªä½ç½®çœŸ 0 ä¸”ç„¡æ¢¯åº¦ï¼Œ"ä¸æœƒé•·å›ä¾†"
        """
        print("[Pruning] zero_lock with reparam (no remove)")
        pruned_total = 0
        total_params = 0

        for name, module in self.model.named_modules():
            if not self._should_prune_layer(name, target_layers):
                continue
            if not hasattr(module, 'weight'):
                continue

            W = module.weight.data
            total_params += W.numel()

            # 1) importance & threshold
            imp = torch.abs(W)
            thr = torch.quantile(imp.flatten(), prune_ratio)
            keep_mask = (imp >= thr).to(W.dtype)  # 1=keep, 0=prune
            prune_mask = (imp < thr)

            # 2) å…ˆæŠŠè¢«å‰ªä½ç½®è¨­ 0ï¼ˆæ–¹ä¾¿è§€å¯Ÿï¼‰
            W.mul_(keep_mask)

            # 3) æ›ä¸Š reparam é®ç½©ï¼ˆä¸ removeï¼‰
            prune.custom_from_mask(module, name='weight', mask=keep_mask)

            # 4)ï¼ˆå¯é¸ï¼‰åªå°ä¿ç•™ä½ç½®åŠ å¾ˆå°å™ªè²ç©©å®šè¨“ç·´
            if noise_std and noise_std > 0:
                with torch.no_grad():
                    std = W[keep_mask.bool()].std().clamp_min(1e-8)
                    noise = torch.randn_like(W) * std * noise_std
                    W.add_(noise * keep_mask)

            pruned_total += prune_mask.sum().item()
            print(f"ä¿®å‰ªå±¤ {name}: {int(prune_mask.sum())}/{W.numel()} ({100*prune_mask.float().mean().item():.1f}%)")

        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_total:,}/{total_params:,} ({100*pruned_total/max(total_params,1):.1f}%)")
        
    def _gradient_based_pruning(self, retain_loader, forget_loader, prune_ratio, target_layers):
        """åŸºæ–¼æ¢¯åº¦çš„ä¿®å‰ªï¼šç§»é™¤å°éºå¿˜æ•¸æ“šæ¢¯åº¦å¤§ä½†å°ä¿ç•™æ•¸æ“šæ¢¯åº¦å°çš„æ¬Šé‡"""
        print("åŸ·è¡ŒåŸºæ–¼æ¢¯åº¦çš„ä¿®å‰ª...")
        
        # è¨ˆç®—å°éºå¿˜æ•¸æ“šçš„æ¢¯åº¦
        forget_gradients = self._compute_gradients(forget_loader, "éºå¿˜æ•¸æ“š")
        
        # è¨ˆç®—å°ä¿ç•™æ•¸æ“šçš„æ¢¯åº¦
        retain_gradients = self._compute_gradients(retain_loader, "ä¿ç•™æ•¸æ“š")
        
        pruned_count = 0
        total_params = 0
        
        for name, module in self.model.named_modules():
            if (self._should_prune_layer(name, target_layers) and 
                hasattr(module, 'weight') and 
                name in forget_gradients and 
                name in retain_gradients):
                
                forget_grad = forget_gradients[name]
                retain_grad = retain_gradients[name]
                
                # è¨ˆç®—éºå¿˜åˆ†æ•¸ï¼šå°éºå¿˜æ•¸æ“šæ•æ„Ÿä½†å°ä¿ç•™æ•¸æ“šä¸æ•æ„Ÿçš„æ¬Šé‡
                forget_score = torch.abs(forget_grad) / (torch.abs(retain_grad) + 1e-8)
                
                # ä¿®å‰ªåˆ†æ•¸æœ€é«˜çš„æ¬Šé‡
                threshold = torch.quantile(forget_score.flatten(), 1 - prune_ratio)
                mask = forget_score < threshold
                
                # çµ±è¨ˆ
                total_params += module.weight.numel()
                pruned_count += (mask == 0).sum().item()
                
                # æ‡‰ç”¨æ©ç¢¼
                module.weight.data *= mask.float()
                
                print(f"æ¢¯åº¦ä¿®å‰ªå±¤ {name}: {(mask == 0).sum().item()}/{module.weight.numel()} "
                      f"({100 * (mask == 0).sum().item() / module.weight.numel():.1f}%)")
        
        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_count:,}/{total_params:,} ({100*pruned_count/total_params:.1f}%)")
    
    def _fisher_based_pruning(self, retain_loader, prune_ratio, target_layers):
        """åŸºæ–¼Fisherä¿¡æ¯çš„ä¿®å‰ªï¼šä¿ç•™å°ä¿ç•™æ•¸æ“šé‡è¦çš„æ¬Šé‡"""
        print("åŸ·è¡ŒåŸºæ–¼Fisherä¿¡æ¯çš„ä¿®å‰ª...")
        
        fisher_info = self._compute_fisher_information(retain_loader)
        
        pruned_count = 0
        total_params = 0
        
        for name, module in self.model.named_modules():
            if (self._should_prune_layer(name, target_layers) and 
                hasattr(module, 'weight') and 
                name in fisher_info):
                
                fisher = fisher_info[name]
                
                # ä¿ç•™Fisherä¿¡æ¯æœ€å¤§çš„æ¬Šé‡
                threshold = torch.quantile(fisher.flatten(), prune_ratio)
                mask = fisher > threshold
                
                # çµ±è¨ˆ
                total_params += module.weight.numel()
                pruned_count += (mask == 0).sum().item()
                
                # æ‡‰ç”¨æ©ç¢¼
                module.weight.data *= mask.float()
                
                print(f"Fisherä¿®å‰ªå±¤ {name}: {(mask == 0).sum().item()}/{module.weight.numel()} "
                      f"({100 * (mask == 0).sum().item() / module.weight.numel():.1f}%)")
        
        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_count:,}/{total_params:,} ({100*pruned_count/total_params:.1f}%)")
    
    def _compute_gradients(self, dataloader, data_type):
        """è¨ˆç®—å°ç‰¹å®šæ•¸æ“šé›†çš„æ¢¯åº¦"""
        print(f"è¨ˆç®—{data_type}çš„æ¢¯åº¦...")
        gradients = {}
        self.model.eval()
        
        batch_count = 0
        for batch in dataloader:
            if batch_count >= 2:  # åªç”¨å‰2å€‹batchè¨ˆç®—ï¼Œç¯€çœæ™‚é–“
                break
                
            inputs, labels = batch
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            
            for name, module in self.model.named_modules():
                if hasattr(module, 'weight') and module.weight.grad is not None:
                    if name not in gradients:
                        gradients[name] = torch.zeros_like(module.weight)
                    gradients[name] += torch.abs(module.weight.grad)
            
            batch_count += 1
        
        # å¹³å‡åŒ–æ¢¯åº¦
        for name in gradients:
            gradients[name] /= batch_count
        
        return gradients
    
    def _compute_fisher_information(self, dataloader):
        """è¨ˆç®—Fisherä¿¡æ¯çŸ©é™£"""
        print("è¨ˆç®—Fisherä¿¡æ¯...")
        fisher_info = {}
        self.model.eval()
        
        batch_count = 0
        for batch in dataloader:
            if batch_count >= 3:  # åªç”¨å‰3å€‹batchè¨ˆç®—
                break
                
            inputs, labels = batch
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            outputs = self.model(inputs)
            
            # å°æ¯å€‹é¡åˆ¥è¨ˆç®—Fisherä¿¡æ¯
            for c in range(outputs.size(1)):
                self.model.zero_grad()
                class_loss = outputs[:, c].sum()
                class_loss.backward(retain_graph=True)
                
                for name, module in self.model.named_modules():
                    if hasattr(module, 'weight') and module.weight.grad is not None:
                        if name not in fisher_info:
                            fisher_info[name] = torch.zeros_like(module.weight)
                        fisher_info[name] += module.weight.grad ** 2
            
            batch_count += 1
        
        # å¹³å‡åŒ–Fisherä¿¡æ¯
        for name in fisher_info:
            fisher_info[name] /= (batch_count * outputs.size(1))
        
        return fisher_info
    
    def _build_scheduler(self, optimizer, scheduler_type, epochs, train_loader, min_lr):
        if scheduler_type == "cosine_warmup":
            total_steps = max(1, epochs * len(train_loader))
            warmup_steps = max(1, int(0.1 * total_steps))  # å‰ 10% steps åš warmup
            base_lr = optimizer.param_groups[0]['lr']
            floor = float(min_lr) / float(base_lr)

            def lr_lambda(step):
                if step < warmup_steps:
                    return float(step + 1) / float(warmup_steps)
                progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))
                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))
                return floor + (1.0 - floor) * cosine

            return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
        elif scheduler_type == "cosine":
            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=min_lr)
        else:
            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=min_lr)

    def _should_prune_layer(self, layer_name, target_layers):
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ä¿®å‰ªè©²å±¤"""
        for target in target_layers:
            if target == 'head' and 'head' in layer_name:
                return True
            elif target == 'late_blocks' and 'blocks' in layer_name:
                # åªä¿®å‰ªå¾ŒåŠéƒ¨åˆ†çš„blocks
                try:
                    if 'blocks.' in layer_name:
                        block_idx = int(layer_name.split('blocks.')[1].split('.')[0])
                        return block_idx >= 5  # ä¿®å‰ªç¬¬5å±¤ä¹‹å¾Œçš„blocks
                except:
                    return False
            elif target == '30%_blocks' and 'blocks' in layer_name:
                # åªä¿®å‰ªå¾Œ30%çš„blocksï¼ˆå‡è¨­ç¸½å…±10å±¤ï¼‰
                try:
                    if 'blocks.' in layer_name:
                        block_idx = int(layer_name.split('blocks.')[1].split('.')[0])
                        return block_idx >= 7  # ä¿®å‰ªç¬¬7å±¤ä¹‹å¾Œçš„blocks
                except:
                    return False
            elif target == 'all_blocks' and 'blocks' in layer_name:
                return True
            elif target in layer_name:
                return True
            
            # å‘½ä¸­ç›®æ¨™å±¤ä¹‹å¾Œï¼Œå†æª¢æŸ¥æ˜¯å¦è©²è·³éï¼ˆLayerNorm / bias / 1D å‘é‡ï¼‰
            # é€™è£¡ç”¨åå­—å¿«é€Ÿç•¥éï¼›å¯¦éš›å¥—ç”¨åœ¨ named_modules() æ™‚æœƒåªå°æœ‰ .weight çš„ module ç”Ÿæ•ˆ
            # if ('norm' in layer_name) or ('ln' in layer_name) or ('bias' in layer_name):
            #     return False
        
        return False
    
    
    def _fine_tune_after_pruning(
            self,
            retain_loader,
            epochs=15,
            lr=3e-4,
            weight_decay=0.05,
            scheduler_type="cosine_warmup",
            min_lr=1e-6,
            use_ema=True,
            ema_decay=0.999,
            use_l2sp=False,
            l2sp_lambda=5e-4,
            l2sp_layers=("head", "blocks.7", "blocks.8", "blocks.9")  # ä¾ä½  ViT depth=10
        ):
        """
        ä¿®å‰ª(reset/zero-lock)å¾Œçš„ç©©å®šå¾®èª¿ï¼š
        - AdamW + CosineWarmup / Cosine
        - å¯é¸ EMAï¼ˆè©•ä¼°/æŒ‘ best ç”¨ EMA æ¬Šé‡ï¼‰
        - å¯é¸ L2-SPï¼šåƒ…ç´„æŸ head + late blocks æœåŸæ¨¡å‹æ¬Šé‡é æ”
        - æ—©åœä¾ retain_acc
        """
        print(f"ä¿®å‰ªå¾Œå¾®èª¿ {epochs} è¼ª... (lr={lr}, wd={weight_decay}, sched={scheduler_type}, ema={use_ema}, l2sp={use_l2sp})")

        # 1) æº–å‚™å¯è¨“ç·´åƒæ•¸
        trainable = [p for p in self.model.parameters() if p.requires_grad]
        if not trainable:
            print("è­¦å‘Šï¼šæ²’æœ‰å¯è¨“ç·´åƒæ•¸ï¼Œè·³éå¾®èª¿")
            return

        optimizer = optim.AdamW(trainable, lr=lr, weight_decay=weight_decay)
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

        # 2) èª¿åº¦å™¨ï¼ˆèˆ‡ GS ä¸€è‡´é‚è¼¯ï¼‰
        scheduler = self._build_scheduler(
            optimizer=optimizer,
            scheduler_type=scheduler_type,
            epochs=epochs,
            train_loader=retain_loader,
            min_lr=min_lr
        )

        # 3) EMAï¼ˆåƒ…åœ¨åƒæ•¸éœ€è¨“ç·´æ™‚å•Ÿç”¨ï¼‰
        ema_shadow = {n: p.detach().clone() for n, p in self.model.named_parameters() if p.requires_grad} if use_ema else None
        def ema_update():
            if ema_shadow is None: return
            for n, p in self.model.named_parameters():
                if p.requires_grad:
                    ema_shadow[n].mul_(ema_decay).add_(p.detach(), alpha=1.0 - ema_decay)

        def ema_apply():
            if ema_shadow is None: return None
            snap = {k: v.detach().clone() for k, v in self.model.state_dict().items()}
            sd = self.model.state_dict()
            for n, p in self.model.named_parameters():
                if p.requires_grad:
                    sd[n].copy_(ema_shadow[n])
            self.model.load_state_dict(sd)
            return snap

        def ema_restore(snapshot):
            if snapshot is None: return
            self.model.load_state_dict(snapshot)

        # 4) ä¾› L2-SP ä½¿ç”¨çš„åŸå§‹æ¬Šé‡ï¼ˆè«‹åœ¨ unlearn() ä¸€é–‹å§‹å…ˆç·©å­˜ï¼šself._orig_sd = clone_state_dict(self.model)ï¼‰
        orig_sd = getattr(self, "_orig_sd", None)

        def l2sp_penalty():
            if (not use_l2sp) or (orig_sd is None):
                return 0.0
            pen = 0.0
            for n, p in self.model.named_parameters():
                if (not p.requires_grad):
                    continue
                if any(tag in n for tag in l2sp_layers):
                    pen = pen + (p - orig_sd[n].to(p.device)).pow(2).sum()
            return pen * l2sp_lambda

        # 5) æ—©åœ
        best_acc = -1.0
        best_state = None
        patience = 6
        no_improve = 0

        for ep in range(epochs):
            self.model.train()
            total_loss = 0.0
            total = 0
            correct = 0

            pbar = tqdm(retain_loader, desc=f"å¾®èª¿ Epoch {ep+1}/{epochs}")
            for x, y in pbar:
                x, y = x.to(self.device), y.to(self.device)

                optimizer.zero_grad()
                logits = self.model(x)
                ce_loss = criterion(logits, y)

                # ç†±åº¦/æ ¡æº–ï¼šå°ç†µæ­£å‰‡ï¼ˆæŠ‘åˆ¶éåº¦è‡ªä¿¡ï¼‰
                probs = F.softmax(logits, dim=1)
                entropy_reg = -(probs * (probs + 1e-12).log()).sum(dim=1).mean()
                loss = ce_loss + 1e-3 * entropy_reg

                # å¹¾ä½•ç©©å®šï¼šL2-SPï¼ˆåƒ… head+late blocksï¼‰
                l2sp = l2sp_penalty()
                if l2sp != 0.0:
                    loss = loss + l2sp

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                if scheduler_type in {"onecycle", "cosine_warmup"}:
                    scheduler.step()

                # EMA
                ema_update()

                with torch.no_grad():
                    pred = logits.argmax(1)
                    correct += (pred == y).sum().item()
                    total += y.size(0)
                    total_loss += loss.item()
                pbar.set_postfix({'loss': f"{loss.item():.3f}", 'acc': f"{100*correct/max(total,1):.2f}%"})

            train_loss = total_loss / max(1, len(retain_loader))
            train_acc = 100.0 * correct / max(1, total)

            # ç”¨ EMA æ¬Šé‡é©—è­‰ â†’ å†é‚„åŸ
            snapshot = ema_apply()
            val_acc = self._evaluate_retain(self.model, retain_loader)
            ema_restore(snapshot)

            if scheduler_type in {"cosine", "step"}:
                scheduler.step()
            # plateau çš„è©±åŠ ä¸Š elif scheduler_type == "plateau": scheduler.step(val_acc)

            print(f"å¾®èª¿ Epoch {ep+1}: Loss={train_loss:.4f}, Train_Acc={train_acc:.2f}%, Val_Acc={val_acc:.2f}%")

            if val_acc > best_acc:
                best_acc = val_acc
                best_state = {k: v.detach().clone().cpu() for k, v in self.model.state_dict().items()}
                no_improve = 0
                print(f"  [New Best] retain_acc={val_acc:.2f}%")
            else:
                no_improve += 1
                if no_improve >= patience:
                    print(f"æ—©åœï¼š{patience} å€‹ epoch ç„¡æå‡")
                    break

        if best_state is not None:
            self.model.load_state_dict(best_state)
        print(f"å¾®èª¿å®Œæˆï¼Œbest retain_acc={best_acc:.2f}%")

    def _evaluate_retain(self, model, retain_loader):
        """è©•ä¼°ä¿ç•™é›†æº–ç¢ºç‡"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in retain_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return 100. * correct / total
    
    def get_strategy_configs(self, prune_ratios=None, target_layers=None):
        """ç²å–æ‰€æœ‰å¯ç”¨çš„ç­–ç•¥é…ç½®"""
        configs = {}
        
        if prune_ratios is None or len(prune_ratios) == 0:
            prune_ratios = [0.05, 0.1, 0.2, 0.3]
        
        # Magnitude-based strategies
        for ratio in prune_ratios:
            # configs[f'magnitude_{int(ratio*100)}%'] = {
            #     'prune_ratio': ratio,
            #     'prune_strategy': 'magnitude',
            #     # 'target_layers': ['head', 'all_blocks'],
            #     'target_layers': target_layers,
            # }

            configs[f'magnitude_zero_lock_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'magnitude_zero_lock',
                'prune_mode': 'zero_lock',
                'target_layers': target_layers,
                # 'target_layers': ['head', 'all_blocks']
            }

            configs[f'magnitude_reset_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'magnitude_reset',
                'prune_mode': 'reset',        # â† reset ç‰ˆ
                'target_layers': target_layers,
                # 'target_layers': ['head', 'all_blocks']
            }
        
        # Gradient-based strategies  
        for ratio in prune_ratios:
            configs[f'gradient_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'gradient',
                # 'target_layers': ['head', 'all_blocks'],
                'target_layers': target_layers,
            }
        
        # Fisher-based strategies
        for ratio in prune_ratios:
            configs[f'fisher_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'fisher',
                # 'target_layers': ['head', 'all_blocks'],
                'target_layers': target_layers,
            }
        
        return configs
