"""
WeightPruning éºå¿˜å™¨ - æ™ºèƒ½æ¬Šé‡ä¿®å‰ªç­–ç•¥
"""
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.nn.utils.prune as prune
import numpy as np
from tqdm import tqdm
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.base_unlearner import BaseUnlearner


class WeightPruningUnlearner(BaseUnlearner):
    """
    æ¬Šé‡ä¿®å‰ªéºå¿˜ï¼šç§»é™¤èˆ‡éºå¿˜æ•¸æ“šç›¸é—œçš„æ¬Šé‡
    
    ç”¨é€”èˆ‡æ„ç¾©ï¼š
    1. ç›´æ¥ç§»é™¤èˆ‡éºå¿˜æ•¸æ“šç›¸é—œçš„ç¥ç¶“é€£æ¥
    2. ç ”ç©¶æ¬Šé‡é‡è¦æ€§èˆ‡éºå¿˜çš„é—œä¿‚
    3. å¯¦ç¾æ°¸ä¹…æ€§çš„çŸ¥è­˜ç§»é™¤
    """
    
    def unlearn(self, 
                retain_loader, 
                forget_loader, 
                prune_ratio=0.1, 
                prune_strategy='magnitude_reset', 
                target_layers=['head', 'late_blocks'],
                fine_tune_epochs=15,
                prune_mode='reset'
    ):
        """
        æ¬Šé‡ä¿®å‰ªéºå¿˜ä¸»æ–¹æ³•
        
        Args:
            retain_loader: ä¿ç•™æ•¸æ“šåŠ è¼‰å™¨
            forget_loader: éºå¿˜æ•¸æ“šåŠ è¼‰å™¨
            prune_ratio: ä¿®å‰ªæ¯”ä¾‹
            prune_strategy: ä¿®å‰ªç­–ç•¥ ('magnitude', 'gradient', 'fisher')
            target_layers: ç›®æ¨™å±¤
            fine_tune_epochs: ä¿®å‰ªå¾Œå¾®èª¿è¼ªæ•¸
        """
        
        print(f"é–‹å§‹æ¬Šé‡ä¿®å‰ªéºå¿˜ï¼Œç­–ç•¥: {prune_strategy}, ä¿®å‰ªæ¯”ä¾‹: {prune_ratio}")
        print("ä¿æŒåŸå§‹ 100 é¡åˆ†é¡é ­ï¼Œä¸é€²è¡Œé¡åˆ¥æ˜ å°„")
        
        if prune_strategy == 'magnitude_zero_lock':
            self._magnitude_prune_zero_lock(prune_ratio, target_layers)
        elif prune_strategy == 'magnitude_reset':
            self._magnitude_prune_reset(prune_ratio, target_layers)
        elif prune_strategy == 'gradient':
            self._gradient_based_pruning(retain_loader, forget_loader, prune_ratio, target_layers)
        elif prune_strategy == 'fisher':
            self._fisher_based_pruning(retain_loader, prune_ratio, target_layers)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ä¿®å‰ªç­–ç•¥: {prune_strategy}")
        
        # ä¿®å‰ªå¾Œå¾®èª¿
        if retain_loader is not None and fine_tune_epochs > 0:
            self._fine_tune_after_pruning(retain_loader, fine_tune_epochs)
        
        return self.model
    
    def _magnitude_prune_reset(self, prune_ratio, target_layers):
        """
        reset ç‰ˆï¼šå°æ¬Šé‡ => ç›´æ¥ç½® 0ï¼Œä¸æ› maskï¼Œå¾ŒçºŒå¯è¢«æ¢¯åº¦æ›´æ–°
        """
        print("[Pruning] reset mode (no mask; weights can grow again)")
        pruned_total = 0
        total_params = 0

        for name, module in self.model.named_modules():
            if not self._should_prune_layer(name, target_layers): 
                continue
            # if not isinstance(module, torch.nn.Linear):
            #     continue
            # if self._should_skip_param(name, module):
            #     continue
            if not hasattr(module, 'weight'):
                continue

            W = module.weight.data
            total_params += W.numel()

            imp = torch.abs(W)
            thr = torch.quantile(imp.flatten(), prune_ratio)
            keep_mask = (imp >= thr)
            prune_mask = ~keep_mask

            # 1) ç›´æ¥æŠŠè¢«å‰ªä½ç½®è¨­ç‚º 0ï¼ˆä¸æ›ä»»ä½• maskï¼‰
            W[prune_mask] = 0.0

            pruned_total += prune_mask.sum().item()
            ratio = 100 * prune_mask.float().mean().item()
            print(f"ä¿®å‰ªå±¤ {name}: {int(prune_mask.sum())}/{W.numel()} ({ratio:.1f}%)")

        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_total:,}/{total_params:,} ({100*pruned_total/max(total_params,1):.1f}%)")
        
    def _magnitude_prune_zero_lock(self, prune_ratio, target_layers, noise_std=0.0):
        """
        magnitude æ±ºå®šé®ç½© + torch.prune æ› reparamï¼ˆä¸ removeï¼‰
        âœ forward/ backward æ°¸é ç”¨ weight_orig * maskï¼Œè¢«å‰ªä½ç½®çœŸ 0 ä¸”ç„¡æ¢¯åº¦ï¼Œ"ä¸æœƒé•·å›ä¾†"
        """
        print("[Pruning] zero_lock with reparam (no remove)")
        pruned_total = 0
        total_params = 0

        for name, module in self.model.named_modules():
            if not self._should_prune_layer(name, target_layers):
                continue
            if not hasattr(module, 'weight'):
                continue

            W = module.weight.data
            total_params += W.numel()

            # 1) importance & threshold
            imp = torch.abs(W)
            thr = torch.quantile(imp.flatten(), prune_ratio)
            keep_mask = (imp >= thr).to(W.dtype)  # 1=keep, 0=prune
            prune_mask = (imp < thr)

            # 2) å…ˆæŠŠè¢«å‰ªä½ç½®è¨­ 0ï¼ˆæ–¹ä¾¿è§€å¯Ÿï¼‰
            W.mul_(keep_mask)

            # 3) æ›ä¸Š reparam é®ç½©ï¼ˆä¸ removeï¼‰
            prune.custom_from_mask(module, name='weight', mask=keep_mask)

            # 4)ï¼ˆå¯é¸ï¼‰åªå°ä¿ç•™ä½ç½®åŠ å¾ˆå°å™ªè²ç©©å®šè¨“ç·´
            if noise_std and noise_std > 0:
                with torch.no_grad():
                    std = W[keep_mask.bool()].std().clamp_min(1e-8)
                    noise = torch.randn_like(W) * std * noise_std
                    W.add_(noise * keep_mask)

            pruned_total += prune_mask.sum().item()
            print(f"ä¿®å‰ªå±¤ {name}: {int(prune_mask.sum())}/{W.numel()} ({100*prune_mask.float().mean().item():.1f}%)")

        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_total:,}/{total_params:,} ({100*pruned_total/max(total_params,1):.1f}%)")
        
    def _gradient_based_pruning(self, retain_loader, forget_loader, prune_ratio, target_layers):
        """åŸºæ–¼æ¢¯åº¦çš„ä¿®å‰ªï¼šç§»é™¤å°éºå¿˜æ•¸æ“šæ¢¯åº¦å¤§ä½†å°ä¿ç•™æ•¸æ“šæ¢¯åº¦å°çš„æ¬Šé‡"""
        print("åŸ·è¡ŒåŸºæ–¼æ¢¯åº¦çš„ä¿®å‰ª...")
        
        # è¨ˆç®—å°éºå¿˜æ•¸æ“šçš„æ¢¯åº¦
        forget_gradients = self._compute_gradients(forget_loader, "éºå¿˜æ•¸æ“š")
        
        # è¨ˆç®—å°ä¿ç•™æ•¸æ“šçš„æ¢¯åº¦
        retain_gradients = self._compute_gradients(retain_loader, "ä¿ç•™æ•¸æ“š")
        
        pruned_count = 0
        total_params = 0
        
        for name, module in self.model.named_modules():
            if (self._should_prune_layer(name, target_layers) and 
                hasattr(module, 'weight') and 
                name in forget_gradients and 
                name in retain_gradients):
                
                forget_grad = forget_gradients[name]
                retain_grad = retain_gradients[name]
                
                # è¨ˆç®—éºå¿˜åˆ†æ•¸ï¼šå°éºå¿˜æ•¸æ“šæ•æ„Ÿä½†å°ä¿ç•™æ•¸æ“šä¸æ•æ„Ÿçš„æ¬Šé‡
                forget_score = torch.abs(forget_grad) / (torch.abs(retain_grad) + 1e-8)
                
                # ä¿®å‰ªåˆ†æ•¸æœ€é«˜çš„æ¬Šé‡
                threshold = torch.quantile(forget_score.flatten(), 1 - prune_ratio)
                mask = forget_score < threshold
                
                # çµ±è¨ˆ
                total_params += module.weight.numel()
                pruned_count += (mask == 0).sum().item()
                
                # æ‡‰ç”¨æ©ç¢¼
                module.weight.data *= mask.float()
                
                print(f"æ¢¯åº¦ä¿®å‰ªå±¤ {name}: {(mask == 0).sum().item()}/{module.weight.numel()} "
                      f"({100 * (mask == 0).sum().item() / module.weight.numel():.1f}%)")
        
        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_count:,}/{total_params:,} ({100*pruned_count/total_params:.1f}%)")
    
    def _fisher_based_pruning(self, retain_loader, prune_ratio, target_layers):
        """åŸºæ–¼Fisherä¿¡æ¯çš„ä¿®å‰ªï¼šä¿ç•™å°ä¿ç•™æ•¸æ“šé‡è¦çš„æ¬Šé‡"""
        print("åŸ·è¡ŒåŸºæ–¼Fisherä¿¡æ¯çš„ä¿®å‰ª...")
        
        fisher_info = self._compute_fisher_information(retain_loader)
        
        pruned_count = 0
        total_params = 0
        
        for name, module in self.model.named_modules():
            if (self._should_prune_layer(name, target_layers) and 
                hasattr(module, 'weight') and 
                name in fisher_info):
                
                fisher = fisher_info[name]
                
                # ä¿ç•™Fisherä¿¡æ¯æœ€å¤§çš„æ¬Šé‡
                threshold = torch.quantile(fisher.flatten(), prune_ratio)
                mask = fisher > threshold
                
                # çµ±è¨ˆ
                total_params += module.weight.numel()
                pruned_count += (mask == 0).sum().item()
                
                # æ‡‰ç”¨æ©ç¢¼
                module.weight.data *= mask.float()
                
                print(f"Fisherä¿®å‰ªå±¤ {name}: {(mask == 0).sum().item()}/{module.weight.numel()} "
                      f"({100 * (mask == 0).sum().item() / module.weight.numel():.1f}%)")
        
        print(f"ç¸½å…±ä¿®å‰ªåƒæ•¸: {pruned_count:,}/{total_params:,} ({100*pruned_count/total_params:.1f}%)")
    
    def _compute_gradients(self, dataloader, data_type):
        """è¨ˆç®—å°ç‰¹å®šæ•¸æ“šé›†çš„æ¢¯åº¦"""
        print(f"è¨ˆç®—{data_type}çš„æ¢¯åº¦...")
        gradients = {}
        self.model.eval()
        
        batch_count = 0
        for batch in dataloader:
            if batch_count >= 10:  # åªç”¨å‰10å€‹batchè¨ˆç®—ï¼Œç¯€çœæ™‚é–“
                break
                
            inputs, labels = batch
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            self.model.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            
            for name, module in self.model.named_modules():
                if hasattr(module, 'weight') and module.weight.grad is not None:
                    if name not in gradients:
                        gradients[name] = torch.zeros_like(module.weight)
                    gradients[name] += torch.abs(module.weight.grad)
            
            batch_count += 1
        
        # å¹³å‡åŒ–æ¢¯åº¦
        for name in gradients:
            gradients[name] /= batch_count
        
        return gradients
    
    def _compute_fisher_information(self, dataloader):
        """è¨ˆç®—Fisherä¿¡æ¯çŸ©é™£"""
        print("è¨ˆç®—Fisherä¿¡æ¯...")
        fisher_info = {}
        self.model.eval()
        
        batch_count = 0
        for batch in dataloader:
            if batch_count >= 3:  # åªç”¨å‰3å€‹batchè¨ˆç®—
                break
                
            inputs, labels = batch
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            outputs = self.model(inputs)
            
            # å°æ¯å€‹é¡åˆ¥è¨ˆç®—Fisherä¿¡æ¯
            for c in range(outputs.size(1)):
                self.model.zero_grad()
                class_loss = outputs[:, c].sum()
                class_loss.backward(retain_graph=True)
                
                for name, module in self.model.named_modules():
                    if hasattr(module, 'weight') and module.weight.grad is not None:
                        if name not in fisher_info:
                            fisher_info[name] = torch.zeros_like(module.weight)
                        fisher_info[name] += module.weight.grad ** 2
            
            batch_count += 1
        
        # å¹³å‡åŒ–Fisherä¿¡æ¯
        for name in fisher_info:
            fisher_info[name] /= (batch_count * outputs.size(1))
        
        return fisher_info
    
    def _should_prune_layer(self, layer_name, target_layers):
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ä¿®å‰ªè©²å±¤"""
        for target in target_layers:
            if target == 'head' and 'head' in layer_name:
                return True
            # === æ–°å¢ 'late_blocks_mlp' ç›®æ¨™ï¼šåªé‡å°å¾ŒåŠéƒ¨çš„ Block (5-9) MLP å±¤ ===
            elif target == 'late_blocks_mlp' and 'blocks.' in layer_name:
                try:
                    block_idx = int(layer_name.split('blocks.')[1].split('.')[0])
                    is_late_block = block_idx >= 5
                    is_mlp = ('mlp.fc1' in layer_name or 'mlp.fc2' in layer_name)
                    if is_late_block and is_mlp:
                        return True
                except:
                    return False
            # ====================================================================
            
            # === ç¾æœ‰ 'blocks_mlp' ç›®æ¨™ï¼šé‡å°æ‰€æœ‰ Block çš„ MLP å±¤ ===
            elif target == 'blocks_mlp' and 'blocks.' in layer_name and ('mlp.fc1' in layer_name or 'mlp.fc2' in layer_name):
                return True
            # =======================================================
            elif target == 'late_blocks' and 'blocks' in layer_name:
                # åªä¿®å‰ªå¾ŒåŠéƒ¨åˆ†çš„blocks (åŒ…æ‹¬ Attention å’Œ MLP)
                try:
                    if 'blocks.' in layer_name:
                        block_idx = int(layer_name.split('blocks.')[1].split('.')[0])
                        return block_idx >= 5  # ä¿®å‰ªç¬¬5å±¤ï¼ˆç´¢å¼• 5ï¼‰ä¹‹å¾Œçš„blocks (5, 6, 7, 8, 9)
                except:
                    return False
            elif target == '30%_blocks' and 'blocks' in layer_name:
                # åªä¿®å‰ªå¾Œ30%çš„blocksï¼ˆå‡è¨­ç¸½å…±10å±¤ï¼‰
                try:
                    if 'blocks.' in layer_name:
                        block_idx = int(layer_name.split('blocks.')[1].split('.')[0])
                        return block_idx >= 7  # ä¿®å‰ªç¬¬7å±¤ï¼ˆç´¢å¼• 7ï¼‰ä¹‹å¾Œçš„blocks (7, 8, 9)
                except:
                    return False
            elif target == 'all_blocks' and 'blocks' in layer_name:
                return True
            elif target in layer_name:
                return True
        return False
    
    def _should_skip_param(self, name, module):
        # è·³é LayerNorm
        if isinstance(module, torch.nn.LayerNorm):
            return True
        # è·³éä½ç½®/cls token/pos_embed
        if 'pos_embed' in name or 'cls_token' in name:
            return True
        # è·³é patch_embedï¼ˆtoken åŒ–åº•åº§ï¼‰
        if 'patch_embed' in name:
            return True
        # è·³é bias
        #   - module å¯èƒ½æ²’æœ‰ bias æˆ– bias æ˜¯ None
        if hasattr(module, 'bias') and module.bias is not None:
            # è‹¥ç•¶å‰ç›®æ¨™æ˜¯ weightï¼Œåªè¦é€™å±¤æœ‰ biasï¼Œæˆ‘å€‘ä»ç„¶åª prune weightï¼›
            # ä½†ç‚ºé¿å…èª¤å‹• biasï¼Œå¯æ˜ç¢ºåœ¨ä¸‹é¢çš„å±¤å…§æª¢æŸ¥ name.endswith('.bias')
            pass
        return False

    # def _fine_tune_after_pruning(self, retain_loader, epochs):
    #     """ä¿®å‰ªå¾Œçš„å¾®èª¿"""
    #     print(f"ä¿®å‰ªå¾Œå¾®èª¿ {epochs} è¼ª...")
        
    #     # åªè¨“ç·´æœªè¢«å®Œå…¨ä¿®å‰ªçš„åƒæ•¸
    #     trainable_params = []
    #     for name, param in self.model.named_parameters():
    #         if param.requires_grad:
    #             trainable_params.append(param)

    #     if not trainable_params:
    #         print("è­¦å‘Šï¼šæ²’æœ‰å¯è¨“ç·´åƒæ•¸ï¼Œè·³éå¾®èª¿")
    #         return
        
    #     optimizer = optim.AdamW(trainable_params, lr=1e-4, weight_decay=0.01)
    #     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
    #     for epoch in range(epochs):
    #         self.model.train()
    #         total_loss = 0
            
    #         pbar = tqdm(retain_loader, desc=f"å¾®èª¿ Epoch {epoch+1}/{epochs}")
    #         for inputs, labels in pbar:
    #             inputs, labels = inputs.to(self.device), labels.to(self.device)
                
    #             optimizer.zero_grad()
    #             outputs = self.model(inputs)
    #             loss = self.criterion(outputs, labels)
    #             loss.backward()
    #             optimizer.step()
                
    #             total_loss += loss.item()
    #             pbar.set_postfix({'loss': loss.item()})
            
    #         scheduler.step()
            
    #         # è©•ä¼°
    #         val_acc = self._evaluate_retain(self.model, retain_loader)
    #         print(f"å¾®èª¿ Epoch {epoch+1}: Loss={total_loss/len(retain_loader):.4f}, Val_Acc={val_acc:.2f}%")

    def _fine_tune_after_pruning(self, retain_loader, epochs):
        """
        ä¿®å‰ªå¾Œçš„å¾®èª¿ - å°å…¥ OneCycleLR å’Œæ”¹é€²çš„å„ªåŒ–å™¨è¨­å®š
        """
        print(f"ä¿®å‰ªå¾Œå¾®èª¿ {epochs} è¼ª...")
        
        # è¨­ç½®è¶…åƒæ•¸ (å¯ä»¥é€é argparse å‚³å…¥ï¼Œé€™è£¡ä½¿ç”¨å›ºå®šå€¼ä½œç‚ºç¤ºç¯„)
        FT_LR = 3e-4          # ç•¥é«˜æ–¼é è¨­çš„ 1e-4ï¼Œç¢ºä¿æ”¶æ–‚é€Ÿåº¦
        FT_WEIGHT_DECAY = 0.05
        
        # åªè¨“ç·´æœªè¢«å®Œå…¨ä¿®å‰ªçš„åƒæ•¸ï¼ˆå³ `param.requires_grad` ç‚º True çš„æ‰€æœ‰åƒæ•¸ï¼‰
        trainable_params = []
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                trainable_params.append(param)

        if not trainable_params:
            print("è­¦å‘Šï¼šæ²’æœ‰å¯è¨“ç·´åƒæ•¸ï¼Œè·³éå¾®èª¿")
            return
        
        # 1. å„ªåŒ–å™¨ï¼šä½¿ç”¨ AdamWï¼Œç•¥é«˜çš„ Weight Decay æœ‰åŠ©æ–¼æ­£å‰‡åŒ–
        optimizer = optim.AdamW(trainable_params, lr=FT_LR, weight_decay=FT_WEIGHT_DECAY)
        
        # 2. å­¸ç¿’ç‡èª¿åº¦å™¨ï¼šä½¿ç”¨ OneCycleLR (æœ€å¿«ã€æœ€æœ‰æ•ˆç‡çš„å¾®èª¿èª¿åº¦å™¨)
        # ç‚ºäº†ä½¿ç”¨ OneCycleLRï¼Œæˆ‘å€‘éœ€è¦çŸ¥é“ç¸½è¨“ç·´æ­¥æ•¸
        steps_per_epoch = len(retain_loader)
        
        # ç”±æ–¼ FT_LR åªæ˜¯åˆå§‹ LRï¼ŒOneCycleLR çš„ max_lr è¨­ç‚º 10 å€
        max_lr = FT_LR * 10
        scheduler = optim.lr_scheduler.OneCycleLR(
            optimizer, 
            max_lr=max_lr, 
            steps_per_epoch=steps_per_epoch, 
            epochs=epochs,
            anneal_strategy='cos',
            pct_start=0.3
        )
        
        # 3. æå¤±å‡½æ•¸ï¼šå¼•å…¥ Label Smoothing æ­£å‰‡åŒ–ï¼Œä¸¦å¯è€ƒæ…®åŠ å…¥ Logit æ‡²ç½°
        # Label Smoothing: é¿å…éåº¦è‡ªä¿¡ï¼Œæœ‰åŠ©æ–¼æé«˜æ³›åŒ–èƒ½åŠ›
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # è¨˜éŒ„æœ€ä½³æº–ç¢ºåº¦
        best_acc = self._evaluate_retain(self.model, retain_loader)
        best_state = self.model.state_dict().copy() # é¿å…åœ¨ CUDA ä¹‹é–“ç§»å‹•ï¼Œç›´æ¥ copy state_dict
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            
            pbar = tqdm(retain_loader, desc=f"å¾®èª¿ Epoch {epoch+1}/{epochs}")
            for inputs, labels in pbar:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(inputs)
                
                # === æå¤±è¨ˆç®— ===
                loss = criterion(outputs, labels)
                
                # å¯é¸æ­£å‰‡åŒ–ï¼šLogit L2 Norm Penalty
                # if epoch < epochs // 2: # åƒ…åœ¨å‰ä¸€åŠè¼ªæ¬¡æ–½åŠ æ‡²ç½°
                #     logit_norm = outputs.norm(p=2, dim=1).mean()
                #     lambda_logit = 1e-5
                #     loss += lambda_logit * logit_norm

                loss.backward()
                optimizer.step()
                
                # âš ï¸ OneCycleLR å¿…é ˆæ¯æ‰¹æ¬¡ step ä¸€æ¬¡
                scheduler.step()
                
                total_loss += loss.item()
                pbar.set_postfix({'loss': loss.item(), 'lr': optimizer.param_groups[0]['lr']})
            
            # è©•ä¼° (ä½¿ç”¨ mul_vit_1.py ä¸­çš„ _evaluate_retain ä¾†ç¢ºä¿ä¸€è‡´æ€§)
            # ğŸ’¡ é€™è£¡å‘¼å«çš„æ˜¯ Unlearner å…§çš„ _evaluate_retainï¼Œéœ€è¦å‚³å…¥ loader
            val_acc = self._evaluate_retain(self.model, retain_loader)
            current_lr = optimizer.param_groups[0]['lr']
            
            print(f"å¾®èª¿ Epoch {epoch+1}: Loss={total_loss/len(retain_loader):.4f}, Val_Acc={val_acc:.2f}%, LR={current_lr:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_acc:
                best_acc = val_acc
                # åªä¿å­˜ state_dict çš„ CPU ç‰ˆæœ¬ä»¥ç¯€çœé¡¯å­˜
                best_state = {k: v.cpu() for k, v in self.model.state_dict().items()}

        # è¼‰å…¥æœ€ä½³æ¨¡å‹ç‹€æ…‹ï¼ˆé€™å°æ–¼ reset æ¨¡å¼çš„æ¨¡å‹ç©©å®šæ€§éå¸¸é‡è¦ï¼‰
        self.model.load_state_dict(best_state)
        print(f"å¾®èª¿å®Œæˆï¼Œæ¢å¾©æœ€ä½³æ¨¡å‹ (Retain Acc: {best_acc:.2f}%)")
    
    def _evaluate_retain(self, model, retain_loader):
        """è©•ä¼°ä¿ç•™é›†æº–ç¢ºç‡"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in retain_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return 100. * correct / total
    
    def get_strategy_configs(self, prune_ratios=None, target_layers=None):
        """ç²å–æ‰€æœ‰å¯ç”¨çš„ç­–ç•¥é…ç½®"""
        configs = {}
        
        if prune_ratios is None or len(prune_ratios) == 0:
            prune_ratios = [0.05, 0.1, 0.2, 0.3]
        
        # Magnitude-based strategies
        for ratio in prune_ratios:

            configs[f'magnitude_zero_lock_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'magnitude_zero_lock',
                'prune_mode': 'zero_lock',
                'target_layers': target_layers,
            }

            configs[f'magnitude_reset_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'magnitude_reset',
                'prune_mode': 'reset',       
                'target_layers': target_layers,
            }
        
        # Gradient-based strategies  
        for ratio in prune_ratios:
            configs[f'gradient_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'gradient',
                'target_layers': target_layers,
            }
        
        # Fisher-based strategies
        for ratio in prune_ratios:
            configs[f'fisher_{int(ratio*100)}%'] = {
                'prune_ratio': ratio,
                'prune_strategy': 'fisher',
                'target_layers': target_layers,
            }
        
        return configs
