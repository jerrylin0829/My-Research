import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score
from torch.utils.data import DataLoader, Subset, TensorDataset
import os
import time
import json
from tqdm import tqdm
import random
from torchvision import transforms, datasets
from models.vit_LSA_classattn import VisionTransformer
import argparse
from typing import Optional
from torch.utils.tensorboard import SummaryWriter
import copy

def clone_state_dict(obj, to_cpu: bool = True):
    """
    å®‰å…¨æ·±æ‹·è²æ¨¡å‹æ¬Šé‡ã€‚
    å…è¨±å‚³å…¥ nn.Module æˆ– å·²ç¶“æ˜¯ state_dict çš„ dictã€‚
    """
    # è‹¥æ˜¯æ¨¡å‹ï¼Œå…ˆå– state_dict
    sd = obj.state_dict() if hasattr(obj, "state_dict") else obj
    # æ·±æ‹·è²æ¯ä¸€å€‹ tensorï¼›é è¨­æ¬åˆ° CPUï¼Œç¯€çœé¡¯å­˜
    out = {}
    for k, v in sd.items():
        t = v.detach().clone()
        if to_cpu:
            t = t.cpu()
        out[k] = t
    return out

class CosineClassifier(nn.Module):
    def __init__(self, in_dim, num_classes, scale=20.0):
        super().__init__()
        self.W = nn.Parameter(torch.randn(num_classes, in_dim))
        nn.init.xavier_normal_(self.W)
        self.scale = scale
    def forward(self, feats):
        f = F.normalize(feats, dim=1)
        w = F.normalize(self.W, dim=1)
        return self.scale * (f @ w.t())

class EMAHelper:
    def __init__(self, model, decay=0.999):
        self.decay = decay
        self.shadow = {n: p.detach().clone() for n, p in model.named_parameters() if p.requires_grad}
    @torch.no_grad()
    def update(self, model):
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.shadow:
                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1.0 - self.decay)
    @torch.no_grad()
    def apply_to(self, model):
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.shadow:
                p.data.copy_(self.shadow[n].data)

def compute_class_weights_from_subset(subset, num_classes):
    counts = torch.zeros(num_classes, dtype=torch.long)
    ds = subset.dataset
    for idx in subset.indices:
        _, y = ds[idx]
        counts[y] += 1
    counts.clamp_(min=1)
    inv = 1.0 / counts.float()
    return (inv / inv.mean()).float()
         
class MachineUnlearning:
    """
    æ©Ÿå™¨éºå¿˜(Machine Unlearning)æ¡†æ¶ - å¯¦ç¾å’Œè©•ä¼°æ¨¡å‹éºå¿˜åŠŸèƒ½
    åŒ…å«å¤šç¨®è©•ä¼°æŒ‡æ¨™: MIA(æˆå“¡æ¨æ–·æ”»æ“Š)ã€GS(é»ƒé‡‘æ¨™æº–)ç­‰
    """
    def __init__(self, 
                 model_class=VisionTransformer, 
                 model_args=None, 
                 device='cuda',
                 batch_size=128,
                 output_dir='./unlearning_output',
                 log_dir: Optional[str] = None,
                 args=None, 
                 ):
        """
        åˆå§‹åŒ–æ©Ÿå™¨éºå¿˜æ¡†æ¶
        
        Args:
            model_class: æ¨¡å‹é¡
            model_args: æ¨¡å‹åˆå§‹åŒ–åƒæ•¸
            device: è¨ˆç®—è¨­å‚™
            output_dir: è¼¸å‡ºç›®éŒ„
        """
        self.model_class = model_class
        self.model_args = model_args or {}
        self.embed_dim = self.model_args.get('embed_dim', 300)
        self.num_classes = self.model_args.get('num_classes', None)
        self.batch_size = batch_size
        self.device = device
        self.output_dir = output_dir
        self.original_labels_test = None
        
        # è®€å–ï¼ˆè‹¥æœ‰ï¼‰CLI æ——æ¨™ï¼Œçµ¦ GS/è©•ä¼°ç”¨
        self.gs_use_ema = bool(getattr(args, "gs_use_ema", False))
        self.gs_ema_decay = float(getattr(args, "gs_ema_decay", 0.999))
        self.gs_use_class_balance = bool(getattr(args, "gs_use_class_balance", False))
        self.gs_warmup_epochs = int(getattr(args, "gs_warmup_epochs", 5))

        # 10/4 00:40 æ›´æ–°é€™å››è¡Œ
        # self.use_cosine_head = getattr(self, "use_cosine_head", True)
        # self.cosine_scale = getattr(self, "cosine_scale", 20.0)
        # if self.use_cosine_head:
        #     self.head = CosineClassifier(in_dim=self.embed_dim, num_classes=self.num_classes, scale=self.cosine_scale)

        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ï¼ˆæˆ–ä½¿ç”¨å¤–éƒ¨æŒ‡å®šï¼‰TensorBoard ç›®éŒ„
        tb_dir = log_dir if log_dir is not None else os.path.join(self.output_dir, "logs")
        os.makedirs(tb_dir, exist_ok=True)

        # åˆå§‹åŒ–TensorBoard
        self.writer = SummaryWriter(log_dir=tb_dir)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.original_model = None  # åŸå§‹æ¨¡å‹ï¼ˆéºå¿˜å‰ï¼‰
        self.unlearned_model = None  # éºå¿˜å¾Œçš„æ¨¡å‹
        self.retrained_model = None  # é‡æ–°è¨“ç·´çš„æ¨¡å‹ï¼ˆé»ƒé‡‘æ¨™æº–ï¼‰
        self.random_model = None  # éš¨æ©Ÿæ¨¡å‹ï¼ˆæœªç¶“è¨“ç·´ï¼‰

        # è·Ÿè¸ªå¯¦é©—çµæœ
        self.results = {
            'unlearning_time': 0,
            'retraining_time': 0,
            'original_metrics': {},
            'unlearned_metrics': {},
            'retrained_metrics': {},
            'mia_results': {},
            'gs_comparison': {}
        }
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        self.set_seed(42)
    
    @staticmethod
    def set_seed(seed):
        """è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        random.seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    def load_original_model(self, model_path):
        """è¼‰å…¥é è¨“ç·´çš„åŸå§‹æ¨¡å‹"""
        self.original_model = self.model_class(**self.model_args).to(self.device)
        self.original_model.load_state_dict(torch.load(model_path, map_location=self.device))
        print(f"åŸå§‹æ¨¡å‹å·²è¼‰å…¥: {model_path}")
        
        # è¤‡è£½ä¸€ä»½ä½œç‚ºå°‡è¦é€²è¡Œéºå¿˜çš„æ¨¡å‹
        self.unlearned_model = self.model_class(**self.model_args).to(self.device)
        self.unlearned_model.load_state_dict(torch.load(model_path, map_location=self.device))
    
    # åˆå§‹åŒ–éš¨æ©Ÿæ¨¡å‹ï¼Œç”¨æ–¼ zrf æŒ‡æ¨™
    def init_random_model(self):
        """
        åˆå§‹åŒ–éš¨æ©Ÿæ¨¡å‹ï¼ˆä¸è¨“ç·´ï¼‰â€”ç”¨æ–¼æ­£ç¢ºçš„ ZRF è¨ˆç®—
        """
        self.random_model = self.model_class(**self.model_args).to(self.device)
        self.random_model.eval()
        return self.random_model
    
    def prepare_data(self, 
                     dataset_name='CIFAR100', 
                     data_path='./data',
                     forget_classes=None,
                     forget_indices=None
        ):
        """
        æº–å‚™æ•¸æ“šé›†ï¼Œåˆ†å‰²ç‚ºè¨˜ä½å’Œéºå¿˜éƒ¨åˆ†
        
        Args:
            dataset_name: æ•¸æ“šé›†åç¨±
            data_path: æ•¸æ“šé›†è·¯å¾‘
            forget_classes: è¦éºå¿˜çš„é¡åˆ¥é›†åˆ(èˆ‡forget_indicesäºŒé¸ä¸€)
            forget_indices: è¦éºå¿˜çš„æ¨£æœ¬ç´¢å¼•(èˆ‡forget_classesäºŒé¸ä¸€)
        """
        # æ•¸æ“šè½‰æ›
        mean = [0.5071, 0.4867, 0.4408] 
        std = [0.2675, 0.2565, 0.2761]
        
        # å¢å¼·æ•¸æ“šè½‰æ› - ç”¨æ–¼è¨“ç·´
        transform_train = transforms.Compose([
            transforms.RandomResizedCrop(32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),
            transforms.TrivialAugmentWide(),
            # transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),
            transforms.RandomHorizontalFlip(),
            transforms.RandomApply(
            [transforms.ColorJitter(0.4, 0.4, 0.3, 0.1)], 
            p=0.5
        ),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])

        # å…ˆå‰çš„ç‰ˆæœ¬
        # transfrom_train = transforms.Compose([
        #     transforms.RandomCrop(32, padding=4),
        #     transforms.RandomHorizontalFlip(),
        #     transforms.ToTensor(),
        #     transforms.Normalize(mean, std)
        # ])

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])
        
        # åŠ è¼‰æ•¸æ“šé›†
        if dataset_name.upper() == 'CIFAR100':
            train_set = datasets.CIFAR100(root=data_path, train=True, download=True, transform=transform_train)
            test_set = datasets.CIFAR100(root=data_path, train=False, download=True, transform=transform_test)
        else:
            raise ValueError(f"Unsupported dataset: {dataset_name}")
        
        # åˆ†å‰²éºå¿˜é›†å’Œä¿ç•™é›†
        if forget_classes is not None:
            # å…ˆç¢ºä¿å¯ç´¢å¼•ï¼ˆé¿å… set é€ æˆå¾ŒçºŒ tensor indexing å‡ºéŒ¯ï¼‰
            if isinstance(forget_classes, set):
                forget_classes = sorted(list(forget_classes))
            elif isinstance(forget_classes, np.ndarray):
                forget_classes = sorted(list(forget_classes.tolist()))
            else:
                forget_classes = sorted(list(forget_classes))

            # æŒ‰é¡åˆ¥åˆ†å‰²
            retain_indices = [i for i, (_, label) in enumerate(train_set) if label not in forget_classes]
            forget_indices = [i for i, (_, label) in enumerate(train_set) if label in forget_classes]
            
            # æ¸¬è©¦é›†ä¹Ÿéœ€è¦ç›¸æ‡‰åˆ†å‰²
            retain_test_indices = [i for i, (_, label) in enumerate(test_set) if label not in forget_classes]
            forget_test_indices = [i for i, (_, label) in enumerate(test_set) if label in forget_classes]
            
            self.num_retain_classes = len(set(label for _, label in train_set) - set(forget_classes))
            self.total_classes = len(set(label for _, label in train_set))
            self.forget_classes = forget_classes
            
            print(f"éºå¿˜é¡åˆ¥: {forget_classes}")
            print(f"ä¿ç•™é¡åˆ¥æ•¸: {self.num_retain_classes}, éºå¿˜é¡åˆ¥æ•¸: {len(forget_classes)}")
        elif forget_indices is not None:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„ç´¢å¼•
            retain_indices = [i for i in range(len(train_set)) if i not in forget_indices]
            
            # å‡è¨­æ¸¬è©¦é›†éœ€è¦æŒ‰åŸå§‹æ¯”ä¾‹åˆ†å‰²
            retain_test_indices = list(range(len(test_set)))
            forget_test_indices = []
            
            self.num_retain_classes = self.total_classes = len(set(label for _, label in train_set))
            self.forget_classes = None
            
            print(f"éºå¿˜æ¨£æœ¬æ•¸: {len(forget_indices)}, ä¿ç•™æ¨£æœ¬æ•¸: {len(retain_indices)}")
        else:
            raise ValueError("å¿…é ˆæŒ‡å®šforget_classesæˆ–forget_indices")
        
        # å‰µå»ºæ•¸æ“šå­é›†
        self.retain_train_set = Subset(train_set, retain_indices)
        self.forget_train_set = Subset(train_set, forget_indices)
        self.retain_test_set = Subset(test_set, retain_test_indices)
        self.forget_test_set = Subset(test_set, forget_test_indices)

        self.class_mapping = None   # ä¸ä½¿ç”¨é¡åˆ¥æ˜ å°„ï¼Œä¿æŒåŸå§‹100é¡
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        num_workers = 8
        pin = True
        persist = num_workers > 0

        self.retain_train_loader = DataLoader(self.retain_train_set, batch_size=self.batch_size, shuffle=True, 
                                              num_workers=num_workers, pin_memory=pin, persistent_workers=persist)
        self.forget_train_loader = DataLoader(self.forget_train_set, batch_size=self.batch_size, shuffle=True, 
                                              num_workers=num_workers, pin_memory=pin, persistent_workers=persist)
        self.retain_test_loader = DataLoader(self.retain_test_set, batch_size=self.batch_size, shuffle=False, 
                                             num_workers=num_workers, pin_memory=pin, persistent_workers=persist)
        self.forget_test_loader = DataLoader(self.forget_test_set, batch_size=self.batch_size, shuffle=False, 
                                             num_workers=num_workers, pin_memory=pin, persistent_workers=persist)

        print(f"æ•¸æ“šæº–å‚™å®Œæˆ:")
        print(f"  ä¿ç•™è¨“ç·´é›†: {len(self.retain_train_set)} æ¨£æœ¬")
        print(f"  éºå¿˜è¨“ç·´é›†: {len(self.forget_train_set)} æ¨£æœ¬")
        print(f"  ä¿ç•™æ¸¬è©¦é›†: {len(self.retain_test_set)} æ¨£æœ¬")
        print(f"  éºå¿˜æ¸¬è©¦é›†: {len(self.forget_test_set)} æ¨£æœ¬")
    
    def get_scheduler(self, optimizer, scheduler_type, epochs, train_loader=None, min_lr=1e-6, onecycle_max_lr=None):
        """
        æ ¹æ“šæŒ‡å®šçš„èª¿åº¦å™¨é¡å‹è¿”å›å­¸ç¿’ç‡èª¿åº¦å™¨
        
        Args:
            optimizer: å„ªåŒ–å™¨
            scheduler_type: èª¿åº¦å™¨é¡å‹ ('cosine', 'step', 'plateau', 'onecycle')
            epochs: è¨“ç·´è¼ªæ•¸
            train_loader: è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨(OneCycleLRéœ€è¦)
            min_lr: æœ€å°å­¸ç¿’ç‡
        
        Returns:
            lr_scheduler: å­¸ç¿’ç‡èª¿åº¦å™¨
        """
        if scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=epochs, eta_min=min_lr
            )
        elif scheduler_type == 'step':
            return optim.lr_scheduler.StepLR(
                optimizer, step_size=epochs//3, gamma=0.1
            )
        elif scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='max', factor=0.5, patience=5, min_lr=min_lr
            )
        elif scheduler_type == 'onecycle':
            max_lr = onecycle_max_lr if onecycle_max_lr is not None else optimizer.param_groups[0]['lr'] * 10.0
            return optim.lr_scheduler.OneCycleLR(
                optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_loader)
            )
        elif scheduler_type == 'cosine_warmup':
            # å…ˆ warmupï¼Œå† cosineï¼Œåˆ°æœ€å¾Œä¸€æ­¥å›åˆ° eta_min / base_lr çš„æ¯”ä¾‹
            assert train_loader is not None, "cosine_warmup éœ€è¦ train_loader ä¾†è¨ˆç®—ç¸½æ­¥æ•¸"
            total_steps = max(1, epochs * len(train_loader))
            warmup_epochs = int(getattr(self, "gs_warmup_epochs", 5))
            warmup_steps = max(1, warmup_epochs * len(train_loader))
            base_lr = optimizer.param_groups[0]['lr']
            floor = float(min_lr) / float(base_lr)

            def lr_lambda(step):
                if step < warmup_steps:
                    return float(step + 1) / float(warmup_steps)
                progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))
                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))
                return floor + (1.0 - floor) * cosine

            return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
        else:
            raise ValueError(f"Unsupported scheduler type: {scheduler_type}")
    
    def _step_scheduler_per_batch(self, scheduler_type, scheduler):
        return scheduler_type in {'onecycle', 'cosine_warmup'}

    def _step_scheduler_per_epoch(self, scheduler_type):
        return scheduler_type in {'cosine', 'step'}

    def unlearn_by_retraining_head(self, epochs=10, lr=5e-5, lr_scheduler_type='cosine', min_lr=1e-6):
        """
        é€šéé‡æ–°è¨“ç·´é ­éƒ¨å¯¦ç¾æ©Ÿå™¨éºå¿˜
        - å‡çµæ‰€æœ‰å±¤ï¼Œåƒ…é‡æ–°è¨“ç·´åˆ†é¡é ­
        - åªä½¿ç”¨ä¿ç•™æ•¸æ“šé›†é€²è¡Œè¨“ç·´
        
        Args:
            epochs: è¨“ç·´è¼ªæ•¸
            lr: å­¸ç¿’ç‡
            lr_scheduler_type: å­¸ç¿’ç‡èª¿åº¦å™¨é¡å‹
            min_lr: æœ€å°å­¸ç¿’ç‡
        """
        assert self.unlearned_model is not None, "è«‹å…ˆåŠ è¼‰åŸå§‹æ¨¡å‹"
        
        print("é–‹å§‹é ­éƒ¨é‡è¨“ç·´éºå¿˜...")
        start_time = time.time()
        
        # å‡çµæ‰€æœ‰åƒæ•¸
        for param in self.unlearned_model.parameters():
            param.requires_grad = False
        
        # è¨­ç½®é ­éƒ¨åƒæ•¸ç‚ºå¯è¨“ç·´
        for param in self.unlearned_model.head.parameters():
            param.requires_grad = True
        
        # è¨­ç½®å„ªåŒ–å™¨
        optimizer = optim.AdamW(self.unlearned_model.head.parameters(), lr=lr, weight_decay=0.05)
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨
        scheduler = self.get_scheduler(optimizer, lr_scheduler_type, epochs, self.retain_train_loader, min_lr,
                                   onecycle_max_lr=lr*10 if lr_scheduler_type=='onecycle' else None)

        
        # è¨“ç·´å¾ªç’°
        best_acc = 0
        best_state = None
        
        for epoch in range(epochs):
            self.unlearned_model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            for inputs, labels in tqdm(self.retain_train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                new_labels = labels
                
                optimizer.zero_grad()
                outputs = self.unlearned_model(inputs)
                ce_loss = criterion(outputs, labels)

                # === ç†µæ­£å‰‡ ===
                probs = F.softmax(outputs, dim=1)
                entropy_reg = -(probs * probs.clamp_min(1e-12).log()).sum(dim=1).mean()

                # === ç¸½ loss ===
                lambda_entropy = 1e-3  # entropy æ­£å‰‡åŒ–æ¬Šé‡, å»ºè­° 1e-3 ~ 5e-3
                loss = ce_loss + lambda_entropy * entropy_reg

                loss.backward()
                optimizer.step()
                
                # if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):
                #     scheduler.step()
                if self._step_scheduler_per_batch(lr_scheduler_type, scheduler):
                    scheduler.step()

                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += new_labels.size(0)
                correct += predicted.eq(new_labels).sum().item()
            
            train_loss = running_loss / len(self.retain_train_loader)
            train_acc = 100 * correct / total
            
            # è©•ä¼°ä¿ç•™é›†æ€§èƒ½
            retain_acc = self._evaluate_retain(self.unlearned_model)
            
            # ğŸ”§ Cosine/Step åœ¨ epoch çµå°¾ stepï¼›Plateau ç”¨ metric
            if self._step_scheduler_per_epoch(lr_scheduler_type):
                scheduler.step()
            
            # æ›´æ–°å­¸ç¿’ç‡(å¦‚æœä½¿ç”¨plateau)
            if lr_scheduler_type == 'plateau':
                scheduler.step(retain_acc)
            
            # è¨˜éŒ„åˆ°TensorBoard
            self.writer.add_scalar('UnlearnHead/Train_Loss', train_loss, epoch)
            self.writer.add_scalar('UnlearnHead/Train_Acc', train_acc, epoch)
            self.writer.add_scalar('UnlearnHead/Retain_Test_Acc', retain_acc, epoch)
            current_lr = optimizer.param_groups[0]['lr']
            self.writer.add_scalar('UnlearnHead/Learning_Rate', current_lr, epoch)
            
            print(f"Epoch {epoch+1}/{epochs}:")
            print(f"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"  Retain Test Acc: {retain_acc:.2f}%")
            print(f"  Learning Rate: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if retain_acc > best_acc:
                best_acc = retain_acc
                best_state = clone_state_dict(self.unlearned_model)

        # æ¢å¾©æœ€ä½³æ¨¡å‹
        self.unlearned_model.load_state_dict(best_state)
        
        end_time = time.time()
        self.results['unlearning_time'] = end_time - start_time
        print(f"éºå¿˜å®Œæˆ! è€—æ™‚: {self.results['unlearning_time']:.2f} ç§’")
        
        # ä¿å­˜éºå¿˜å¾Œçš„æ¨¡å‹
        torch.save(self.unlearned_model.state_dict(), 
                  f"{self.output_dir}/unlearned_head_retrain_model.pth")
    
    def unlearn_by_negative_gradient(self, epochs=10, lr=1e-5, retain_epochs=5, 
                                   lr_scheduler_type='cosine', min_lr=1e-6):
        """
        é€šéè² æ¢¯åº¦æ›´æ–°å¯¦ç¾æ©Ÿå™¨éºå¿˜
        1. å°éºå¿˜æ•¸æ“šåŸ·è¡Œæ¢¯åº¦ä¸Šå‡(ä½¿æ¨¡å‹"å¿˜è¨˜")
        2. å°ä¿ç•™æ•¸æ“šåŸ·è¡Œæ¨™æº–æ¢¯åº¦ä¸‹é™(ä¿æŒæ€§èƒ½)
        
        Args:
            epochs: è² æ¢¯åº¦è¨“ç·´è¼ªæ•¸
            lr: å­¸ç¿’ç‡
            retain_epochs: ä¿ç•™æ•¸æ“šå¾®èª¿è¼ªæ•¸
            lr_scheduler_type: å­¸ç¿’ç‡èª¿åº¦å™¨é¡å‹
            min_lr: æœ€å°å­¸ç¿’ç‡
        """
        assert self.unlearned_model is not None, "è«‹å…ˆåŠ è¼‰åŸå§‹æ¨¡å‹"
        
        print("é–‹å§‹è² æ¢¯åº¦éºå¿˜...")
        start_time = time.time()
        
        # è§£å‡æ‰€æœ‰åƒæ•¸
        for param in self.unlearned_model.parameters():
            param.requires_grad = True
        
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        optimizer = optim.AdamW(self.unlearned_model.parameters(), lr=lr, weight_decay=0.05)
        
        # è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨
        scheduler = self.get_scheduler(
            optimizer, 
            lr_scheduler_type, 
            epochs + retain_epochs, 
            self.retain_train_loader, 
            min_lr,
            onecycle_max_lr=lr*10 if lr_scheduler_type == 'onecycle' else None
        )
        
        # éšæ®µ1: è² æ¢¯åº¦æ›´æ–°(æ¢¯åº¦ä¸Šå‡)
        for epoch in range(epochs):
            self.unlearned_model.train()
            running_loss = 0.0
            
            for inputs, labels in tqdm(self.forget_train_loader, desc=f"å¿˜è¨˜éšæ®µ {epoch+1}/{epochs}"):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.unlearned_model(inputs)
                ce_loss = criterion(outputs, labels)

                # === ç†µæ­£å‰‡ ===
                probs = F.softmax(outputs, dim=1)
                entropy_reg = -(probs * probs.clamp_min(1e-12).log()).sum(dim=1).mean()

                # === ç¸½ loss ===
                lambda_entropy = 1e-3  # entropy æ­£å‰‡åŒ–æ¬Šé‡, å»ºè­° 1e-3 ~ 5e-3
                loss = ce_loss + lambda_entropy * entropy_reg
                
                # è² æ¢¯åº¦æ›´æ–° - æ¢¯åº¦ä¸Šå‡ä½¿æ¨¡å‹"å¿˜è¨˜"
                (-loss).backward()
                optimizer.step()
                
                running_loss += loss.item()
            
            forget_loss = running_loss / len(self.forget_train_loader)
            
            # è©•ä¼°ä¿ç•™æ•¸æ“šæ€§èƒ½
            retain_acc = self._evaluate_retain(self.unlearned_model)
            
            # æ›´æ–°å­¸ç¿’ç‡
            if lr_scheduler_type != 'plateau':
                scheduler.step()
            else:
                scheduler.step(retain_acc)
            
            # è¨˜éŒ„åˆ°TensorBoard
            self.writer.add_scalar('NegGrad/Forget_Loss', forget_loss, epoch)
            self.writer.add_scalar('NegGrad/Retain_Acc', retain_acc, epoch)
            current_lr = optimizer.param_groups[0]['lr']
            self.writer.add_scalar('NegGrad/Learning_Rate', current_lr, epoch)
            
            print(f"å¿˜è¨˜éšæ®µ Epoch {epoch+1}/{epochs}, Loss: {forget_loss:.4f}")
            print(f"  Retain Test Acc: {retain_acc:.2f}%")
            print(f"  Learning Rate: {current_lr:.6f}")
        
        # éšæ®µ2: ä¿ç•™æ•¸æ“šå¾®èª¿
        print("é–‹å§‹ä¿ç•™æ•¸æ“šå¾®èª¿...")
        for epoch in range(retain_epochs):
            self.unlearned_model.train()
            running_loss = 0.0
            correct = 0
            total = 0
            
            for inputs, labels in tqdm(self.retain_train_loader, desc=f"ä¿ç•™éšæ®µ {epoch+1}/{retain_epochs}"):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.unlearned_model(inputs)
                ce_loss = criterion(outputs, labels)

                # === ç†µæ­£å‰‡ ===
                probs = F.softmax(outputs, dim=1)
                entropy_reg = -(probs * probs.clamp_min(1e-12).log()).sum(dim=1).mean()

                # === ç¸½ loss ===
                lambda_entropy = 1e-3  # entropy æ­£å‰‡åŒ–æ¬Šé‡, å»ºè­° 1e-3 ~ 5e-3
                loss = ce_loss + lambda_entropy * entropy_reg

                # æ¨™æº–æ¢¯åº¦ä¸‹é™
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
            
            retain_loss = running_loss / len(self.retain_train_loader)
            retain_acc = 100 * correct / total
            
            # è©•ä¼°ä¿ç•™æ•¸æ“šæ€§èƒ½
            retain_test_acc = self._evaluate_retain(self.unlearned_model)
            
            # æ›´æ–°å­¸ç¿’ç‡
            if lr_scheduler_type != 'plateau':
                scheduler.step()
            else:
                scheduler.step(retain_test_acc)
            
            # è¨˜éŒ„åˆ°TensorBoard
            self.writer.add_scalar('NegGrad/Retain_Loss', retain_loss, epochs + epoch)
            self.writer.add_scalar('NegGrad/Retain_Train_Acc', retain_acc, epochs + epoch)
            self.writer.add_scalar('NegGrad/Retain_Test_Acc', retain_test_acc, epochs + epoch)
            current_lr = optimizer.param_groups[0]['lr']
            self.writer.add_scalar('NegGrad/Learning_Rate', current_lr, epochs + epoch)
            
            print(f"ä¿ç•™éšæ®µ Epoch {epoch+1}/{retain_epochs}, Loss: {retain_loss:.4f}, Acc: {retain_acc:.2f}%")
            print(f"  Retain Test Acc: {retain_test_acc:.2f}%")
            print(f"  Learning Rate: {current_lr:.6f}")
        
        end_time = time.time()
        self.results['unlearning_time'] = end_time - start_time
        print(f"éºå¿˜å®Œæˆ! è€—æ™‚: {self.results['unlearning_time']:.2f} ç§’")
        
        # ä¿å­˜éºå¿˜å¾Œçš„æ¨¡å‹
        torch.save(self.unlearned_model.state_dict(), 
                  f"{self.output_dir}/unlearned_neg_grad_model.pth")
    
    # === Mixup / CutMix ===
    def _apply_mix_augment(self, inputs, targets, alpha=0.2, use_cutmix=False):
        lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0
        batch_size = inputs.size(0)
        index = torch.randperm(batch_size, device=inputs.device)

        # ğŸ› ï¸ æ–°å¢æœ€å°Î»é™åˆ¶ï¼Œé¿å…éåº¦æ··åˆ
        lam = max(lam, 0.8)  # ç¢ºä¿è‡³å°‘80%æ˜¯åŸå§‹åœ–åƒ

        if use_cutmix:
            # cutmixï¼šéš¨æ©ŸçŸ©å½¢å€å¡Šäº¤æ›
            H, W = inputs.size(2), inputs.size(3)
            cx, cy = np.random.randint(W), np.random.randint(H)
            w = int(W * np.sqrt(1 - lam))
            h = int(H * np.sqrt(1 - lam))
            x0 = np.clip(cx - w // 2, 0, W); x1 = np.clip(cx + w // 2, 0, W)
            y0 = np.clip(cy - h // 2, 0, H); y1 = np.clip(cy + h // 2, 0, H)
            inputs[:, :, y0:y1, x0:x1] = inputs[index, :, y0:y1, x0:x1]
            lam = 1 - ((x1 - x0) * (y1 - y0) / (W * H))
            targets_a, targets_b = targets, targets[index]
            return inputs, targets_a, targets_b, lam
        else:
            # mixupï¼šæ•´å¼µç·šæ€§æ··åˆ
            mixed = lam * inputs + (1 - lam) * inputs[index]
            targets_a, targets_b = targets, targets[index]
            return mixed, targets_a, targets_b, lam
        
    def train_gold_standard(self, epochs=250, 
                            lr=1e-3, 
                            lr_scheduler_type='cosine', 
                            min_lr=1e-6, 
                            weight_decay=0.05,
                            use_mixup=False,
                            use_cutmix=False,
                            mix_alpha=0.2,
                            use_logit_penalty=False,
                            experiment_writer=None):
        """
        è¨“ç·´é»ƒé‡‘æ¨™æº–ï¼ˆGSï¼‰æ¨¡å‹ï¼šåƒ…ç”¨ retain setï¼Œè‡ªé›¶é–‹å§‹è¨“ç·´ï¼Œä½œç‚ºå°é½Šèˆ‡æ¯”è¼ƒåŸºæº–ã€‚
        - æ”¯æ´ Class-balanced CEï¼ˆåƒ…å° retain è¨“ç·´é›†ï¼‰
        - æ”¯æ´ EMAï¼ˆæ¯ step æ›´æ–°ï¼›é©—è­‰å‰å¥—ç”¨ï¼›é©—è­‰å¾Œé‚„åŸï¼›æœ€ä½³ä»¥ EMA æ¬Šé‡ä¿å­˜ï¼‰
        - æ”¯æ´ Cosine + Warmupï¼ˆlr_scheduler_type='cosine_warmup'ï¼›æ¯ batch stepï¼‰
        - Mixup/CutMix ä½¿ç”¨è»Ÿæº–ç¢ºç‡ï¼›å¦å‰‡ç”¨æ¨™æº–æº–ç¢ºç‡
        """
        print("é–‹å§‹è¨“ç·´é»ƒé‡‘æ¨™æº–æ¨¡å‹...")
        start_time = time.time()
        if not hasattr(self, "results"):
            self.results = {}

        # 1) å»ºç«‹å…¨æ–°æ¨¡å‹ï¼ˆGSï¼‰ä¸¦ç§»å‹•åˆ° device
        self.retrained_model = self.model_class(**self.model_args).to(self.device)

        # 2) Optimizer / Criterionï¼ˆClass-balanced CE åƒ…å° retain è¨“ç·´é›†ï¼‰
        optimizer = optim.AdamW(self.retrained_model.parameters(), lr=lr, weight_decay=weight_decay)

        use_class_balance = bool(getattr(self, "gs_use_class_balance", False))
        if use_class_balance:
            cw = compute_class_weights_from_subset(self.retain_train_set, self.num_classes).to(self.device)
            criterion = nn.CrossEntropyLoss(weight=cw, label_smoothing=0.1)
        else:
            criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

        # 3) Schedulerï¼ˆæ”¯æ´ cosine_warmup / onecycle éœ€è¦ train_loaderï¼‰
        scheduler = self.get_scheduler(
            optimizer, lr_scheduler_type, epochs, self.retain_train_loader, min_lr,
            onecycle_max_lr=lr*10 if lr_scheduler_type == 'onecycle' else None
        )

        # 4) EMA
        use_ema = bool(getattr(self, "gs_use_ema", False))
        ema_decay = float(getattr(self, "gs_ema_decay", 0.999))
        ema_helper = EMAHelper(self.retrained_model, decay=ema_decay) if use_ema else None

        # 5) è¨“ç·´æ§åˆ¶
        best_acc = 0.0
        best_state = None
        patience = 20
        no_improve = 0

        for epoch in range(epochs):
            self.retrained_model.train()
            running_loss = 0.0
            total = 0
            correct_sum = 0.0  # ç´¯ç© batch-wise æº–ç¢ºç‡ï¼ˆå¯èƒ½æ˜¯è»Ÿæº–ç¢ºç‡ï¼‰
            
            for inputs, labels in tqdm(self.retain_train_loader, desc=f"GS Epoch {epoch+1}/{epochs}"):
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                optimizer.zero_grad()

                # === å‰å‘ï¼šæ˜¯å¦ä½¿ç”¨ Mixup/CutMix ===
                if use_mixup or use_cutmix:
                    inputs, ta, tb, lam = self._apply_mix_augment(inputs, labels, alpha=mix_alpha, use_cutmix=use_cutmix)
                    outputs = self.retrained_model(inputs)
                    ce_loss = lam * criterion(outputs, ta) + (1.0 - lam) * criterion(outputs, tb)

                    with torch.no_grad():
                        pred = outputs.argmax(dim=1)
                        acc_a = (pred == ta).float().mean()
                        acc_b = (pred == tb).float().mean()
                        batch_acc = (lam * acc_a + (1.0 - lam) * acc_b).item()
                else:
                    outputs = self.retrained_model(inputs)
                    ce_loss = criterion(outputs, labels)
                    with torch.no_grad():
                        pred = outputs.argmax(dim=1)
                        batch_acc = (pred == labels).float().mean().item()

                # === æ­£å‰‡ï¼šç†µæ­£å‰‡ï¼ˆåå°ï¼Œé¿å…éåº¦è‡ªä¿¡ï¼‰ï¼‹ï¼ˆå¯é¸ï¼‰logit ç¯„æ•¸æ‡²ç½° ===
                probs = F.softmax(outputs, dim=1)
                entropy_reg = -(probs * probs.clamp_min(1e-12).log()).sum(dim=1).mean()
                lambda_entropy = 1e-3

                if use_logit_penalty:
                    logit_norm = outputs.norm(p=2, dim=1).mean()
                    lambda_logit = 1e-4
                    loss = ce_loss + lambda_entropy * entropy_reg + lambda_logit * logit_norm
                else:
                    loss = ce_loss + lambda_entropy * entropy_reg

                # === åå‘å‚³æ’­ ===
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.retrained_model.parameters(), max_norm=1.0)
                optimizer.step()

                # === EMAï¼šæ¯æ­¥æ›´æ–° ===
                if ema_helper is not None:
                    ema_helper.update(self.retrained_model)

                # === Schedulerï¼šbatch-step çš„é¡å‹ï¼ˆonecycle / cosine_warmupï¼‰åœ¨é€™è£¡ step ===
                if self._step_scheduler_per_batch(lr_scheduler_type, scheduler):
                    scheduler.step()

                running_loss += loss.item()
                total += labels.size(0)
                correct_sum += batch_acc * labels.size(0)

            # === Epoch çµ±è¨ˆ ===
            train_loss = running_loss / max(1, len(self.retain_train_loader))
            train_acc = 100.0 * correct_sum / max(1, total)

            # === é©—è­‰å‰ï¼šè‹¥æœ‰ EMAï¼Œå…ˆå¥—ç”¨åˆ°æ¨¡å‹ï¼›é©—è­‰å¾Œé‚„åŸ ===
            if ema_helper is not None:
                current_state = {k: v.detach().clone() for k, v in self.retrained_model.state_dict().items()}
                ema_helper.apply_to(self.retrained_model)

            retain_acc = self._evaluate_retain(self.retrained_model)

            # ä»¥ç•¶å‰ç”Ÿæ•ˆçš„æ¬Šé‡ï¼ˆè‹¥æœ‰ EMA å³ç‚º EMA æ¬Šé‡ï¼‰åˆ¤æ–·æœ€ä½³ï¼Œä¸¦ä¿å­˜ã€ŒEMA æ¬Šé‡ã€ç‰ˆæœ¬
            if retain_acc > best_acc:
                best_acc = retain_acc
                best_state = {k: v.detach().clone().cpu() for k, v in self.retrained_model.state_dict().items()}
                no_improve = 0
                print(f"  [New Best] Acc: {retain_acc:.2f}%")
            else:
                no_improve += 1

            # é‚„åŸå›å³æ™‚æ¬Šé‡ï¼ˆä»¥ä¾¿ä¸‹ä¸€å€‹ epoch ç¹¼çºŒè¨“ç·´ï¼‰
            if ema_helper is not None:
                self.retrained_model.load_state_dict(current_state)

            # === Schedulerï¼šepoch-step çš„é¡å‹ï¼ˆcosine/stepï¼‰æˆ– plateau åœ¨é€™è£¡ step ===
            if self._step_scheduler_per_epoch(lr_scheduler_type):
                scheduler.step()
            elif lr_scheduler_type == 'plateau':
                scheduler.step(retain_acc)

            # === TensorBoard è¨˜éŒ„ ===
            writer = experiment_writer if experiment_writer is not None else getattr(self, "writer", None)
            if writer is not None:
                writer.add_scalar('GoldStandard_Training/Train_Loss', train_loss, epoch)
                writer.add_scalar('GoldStandard_Training/Train_Acc', train_acc, epoch)
                writer.add_scalar('GoldStandard_Training/Retain_Test_Acc', retain_acc, epoch)
                current_lr = optimizer.param_groups[0]['lr']
                writer.add_scalar('GoldStandard_Training/Learning_Rate', current_lr, epoch)

            print(f"GS Epoch {epoch+1}/{epochs}:")
            print(f"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"  Retain Test Acc (EMA eval={'on' if ema_helper else 'off'}): {retain_acc:.2f}%")
            print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")

            # æ—©åœ
            if no_improve >= patience:
                print(f"æ—©åœ: {patience} è¼ªæ²’æœ‰æ”¹å–„")
                break

        # === è¼‰å…¥æœ€ä½³ï¼ˆEMA æ¬Šé‡ç‰ˆæœ¬ï¼‰ ===
        if best_state is not None:
            self.retrained_model.load_state_dict(best_state)

        end_time = time.time()
        self.results['retraining_time'] = end_time - start_time
        print(f"é»ƒé‡‘æ¨™æº–æ¨¡å‹è¨“ç·´å®Œæˆ! è€—æ™‚: {self.results['retraining_time']:.2f} ç§’")

        # ä¿å­˜ GS æ¬Šé‡
        try:
            torch.save(self.retrained_model.state_dict(), f"{self.output_dir}/gold_standard_model.pth")
        except Exception as e:
            print(f"[WARN] ä¿å­˜ GS æ¬Šé‡å¤±æ•—ï¼š{e}")
    
    def _evaluate_retain(self, model):
        """è©•ä¼°æ¨¡å‹åœ¨ä¿ç•™é›†ä¸Šçš„æ€§èƒ½"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in self.retain_test_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
           
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        return 100 * correct / total
    
    def _evaluate_forget(self, model):
        """
        è©•ä¼°æ¨¡å‹åœ¨éºå¿˜é›†ä¸Šçš„æ€§èƒ½
        """
        if len(self.forget_test_loader.dataset) == 0:
            return None

        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in self.forget_test_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                outputs = model(inputs)
                _, pred = outputs.max(1)

                correct += pred.eq(labels).sum().item()
                total   += labels.size(0)
        
        return 100 * correct / total if total else None
    
    def _forget_proxy_metrics(self, model):
        """
        é¡åˆ¥éºå¿˜æ™‚ï¼Œç”¨ä¸ä¾è³´çœŸå¯¦æ¨™ç±¤çš„ä»£ç†æŒ‡æ¨™è¡¡é‡ã€Œå¿˜è¨˜ç¨‹åº¦ã€ï¼š
        - MSP (Max Softmax Probability) è¶Šä½è¶Šå¥½
        - Entropy è¶Šé«˜è¶Šå¥½
        """
        if len(self.forget_test_loader.dataset) == 0:
            return None
        model.eval()
        
        msp_vals, ent_vals = [], []
        with torch.no_grad():
            for x, _ in self.forget_test_loader:
                x = x.to(self.device)
                logits = model(x)
                probs = F.softmax(logits, dim=1)
                msp = probs.max(dim=1).values
                ent = -(probs * (probs + 1e-12).log()).sum(dim=1)
                msp_vals.append(msp.cpu())
                ent_vals.append(ent.cpu())
        if not msp_vals:
            return None
        msp_mean = torch.cat(msp_vals).mean().item()
        ent_mean = torch.cat(ent_vals).mean().item()
        return {'msp': msp_mean, 'entropy': ent_mean}

    def evaluate_all_models(self):
        """è©•ä¼°æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""
        print("\n===== è©•ä¼°æ‰€æœ‰æ¨¡å‹ =====")
        
        # è©•ä¼°åŸå§‹æ¨¡å‹
        if self.original_model is not None:
            orig_retain_acc = self._evaluate_retain(self.original_model)
            orig_forget_acc = self._evaluate_forget(self.original_model)
            proxy = self._forget_proxy_metrics(self.original_model)
            self.results['original_metrics'] = {
                'retain_acc': orig_retain_acc,
                'forget_acc': orig_forget_acc,
                'forget_proxy': proxy
            }
            print(f"åŸå§‹æ¨¡å‹ - ä¿ç•™é›†: {orig_retain_acc:.2f}%, éºå¿˜é›†: {orig_forget_acc:.2f}%")
            if proxy:
                print(f"  Forget MSP: {proxy['msp']:.4f}, Entropy: {proxy['entropy']:.4f}")
        
        # è©•ä¼°éºå¿˜å¾Œçš„æ¨¡å‹
        if self.unlearned_model is not None:
            unl_retain_acc = self._evaluate_retain(self.unlearned_model)
            unl_forget_acc = self._evaluate_forget(self.unlearned_model)
            proxy = self._forget_proxy_metrics(self.unlearned_model)
            self.results['unlearned_metrics'] = {
                'retain_acc': unl_retain_acc,
                'forget_acc': unl_forget_acc,
                'forget_proxy': proxy
            }
            print(f"éºå¿˜æ¨¡å‹ - ä¿ç•™é›†: {unl_retain_acc:.2f}%, éºå¿˜é›†: {unl_forget_acc if unl_forget_acc is not None else 'N/A'}%")
            if proxy:
                print(f"  Forget MSP: {proxy['msp']:.4f}, Entropy: {proxy['entropy']:.4f}")
        # é»ƒé‡‘æ¨™æº–
        if self.retrained_model is not None:
            gs_retain_acc = self._evaluate_retain(self.retrained_model)
            gs_forget_acc = self._evaluate_forget(self.retrained_model)
            proxy = self._forget_proxy_metrics(self.retrained_model)
            self.results['retrained_metrics'] = {
                'retain_acc': gs_retain_acc,
                'forget_acc': gs_forget_acc,
                'forget_proxy': proxy
            }
            print(f"é»ƒé‡‘æ¨™æº– - ä¿ç•™é›†: {gs_retain_acc:.2f}%, éºå¿˜é›†: {gs_forget_acc if gs_forget_acc is not None else 'N/A'}%")
            if proxy:
                print(f"  Forget MSP: {proxy['msp']:.4f}, Entropy: {proxy['entropy']:.4f}")
    
    # 1) æª¢æŸ¥ head ç¶­åº¦ã€æ˜¯å¦ 100 é¡  # // NEW
    def check_head_integrity(self, model):
        head = getattr(model, 'head', None)
        ok = True
        if head is None:
            print("[Head Check] æ¨¡å‹æ²’æœ‰ head å±¤å±¬æ€§")
            ok = False
        else:
            out_features = getattr(head, 'out_features', None)
            in_features  = getattr(head, 'in_features', None)
            print(f"[Head Check] head.in_features={in_features}, head.out_features={out_features}, num_classes(meta)={getattr(self,'num_classes',None)}")
            if out_features is not None and hasattr(self, 'num_classes'):
                if out_features != self.num_classes:
                    print("[Head Check][è­¦å‘Š] head è¼¸å‡ºç¶­åº¦èˆ‡ num_classes ä¸ä¸€è‡´ï¼")
                    ok = False
        return ok

    # 2) æª¢æŸ¥ head.weight å°å„é¡çš„æ•´åˆ—æ˜¯å¦å…¨ 0ï¼ˆzero-lock çš„ç›´æ¥æŒ‡æ¨™ï¼‰  # // NEW
    def check_head_zero_columns(self, model, forget_classes):
        head = getattr(model, 'head', None)
        if head is None or not hasattr(head, 'weight'):
            print("[Head Zeros] ç„¡æ³•æª¢æŸ¥ï¼ˆæ²’æœ‰ head.weightï¼‰")
            return None
        
        W = head.weight.detach().cpu()  # [C, D]
        col_zero = (W.abs().sum(dim=1) == 0)  # æ¯ä¸€åˆ—æ˜¯å¦å…¨ 0ï¼ˆå°æ‡‰ä¸€å€‹é¡åˆ¥ï¼‰
        num_zero = int(col_zero.sum().item())
        num_total = W.size(0)
        print(f"[Head Zeros] å…¨ 0 åˆ—ï¼š{num_zero}/{num_total} ({100.0*num_zero/num_total:.2f}%)")

        if forget_classes is not None:
            # å¼·å¥è™•ç†ï¼šæŠŠä»»ä½•å‹åˆ¥è½‰æˆå¯ç´¢å¼•çš„ LongTensor
            if isinstance(forget_classes, (set, list, tuple, np.ndarray)):
                idx = torch.tensor(sorted(list(forget_classes)), dtype=torch.long)
            elif torch.is_tensor(forget_classes):
                idx = forget_classes.to(dtype=torch.long, device=col_zero.device).cpu()
            else:
                raise TypeError(f"forget_classes å‹åˆ¥ä¸æ”¯æ´ç´¢å¼•: {type(forget_classes)}")

            # col_zero æ˜¯ CPU tensorï¼Œç¢ºä¿ idx åœ¨ CPU
            idx = idx.cpu()
            forget_zeros = int(col_zero.index_select(dim=0, index=idx).sum().item())
            print(f"[Head Zeros] å¿˜è¨˜é¡ä¸­å…¨ 0 åˆ—ï¼š{forget_zeros}/{len(idx)} ({100.0*forget_zeros/max(len(idx),1):.2f}%)")

        return col_zero

    # 3) å¿˜è¨˜æ¸¬è©¦é›†ï¼šé æ¸¬è½åœ¨ã€Œå¿˜è¨˜é¡ vs ä¿ç•™é¡ã€çš„æ¯”ä¾‹  # // NEW
    def debug_forget_prediction_stats(self, model):
        assert hasattr(self, 'forget_classes') and self.forget_classes is not None, "æ²’æœ‰ forget_classes å¯ç”¨"
        fc = set(self.forget_classes)
        model.eval()
        total, pred_in_forget, pred_in_retain = 0, 0, 0
        with torch.no_grad():
            for x, _ in self.forget_test_loader:
                x = x.to(self.device)
                pred = model(x).argmax(dim=1).cpu().tolist()
                for p in pred:
                    if p in fc: pred_in_forget += 1
                    else:       pred_in_retain += 1
                total += len(pred)
        r_forget = pred_in_forget / max(total,1)
        r_retain = pred_in_retain / max(total,1)
        print(f"[Forget Pred Stats] total={total}, pred_in_forget={pred_in_forget} ({r_forget:.3f}), pred_in_retain={pred_in_retain} ({r_retain:.3f})")
        return r_forget, r_retain

    # 4) å¿˜è¨˜æ¸¬è©¦é›†ï¼šçœŸå¯¦é¡ï¼ˆå±¬æ–¼å¿˜è¨˜ï¼‰çš„ logit ç›´æ–¹åœ–ï¼ˆæ•¸æ“šåŒ– + åœ–æª”ï¼‰  # // NEW
    def plot_true_class_logit_hist(self, model, tag, out_dir):
        import os, torch
        import matplotlib.pyplot as plt
        os.makedirs(out_dir, exist_ok=True)
        model.eval()
        vals = []
        with torch.no_grad():
            for x, y in self.forget_test_loader:
                x = x.to(self.device); y = y.to(self.device)
                z = model(x)  # (B, C)
                true_logit = z.gather(1, y.view(-1,1)).squeeze(1)
                vals.append(true_logit.cpu())
        if not vals:
            print(f"[{tag}] ç„¡å¿˜è¨˜æ¸¬è©¦æ¨£æœ¬ï¼›è·³éç¹ªåœ–")
            return
        v = torch.cat(vals).numpy()
        plt.figure()
        plt.hist(v, bins=50)
        plt.title(f"True-class logits on FORGET ({tag})")
        plt.xlabel("logit (true class)"); plt.ylabel("count")
        fp = os.path.join(out_dir, f"{tag}_forget_trueclass_logit_hist.png")
        plt.savefig(fp, dpi=180); plt.close()
        print(f"[Save] {fp}")

    # 5) å¿˜è¨˜æ¸¬è©¦é›†ï¼šé¡åˆ¥åˆ†ä½ˆï¼ˆé æ¸¬å‡ºç¾æ¬¡æ•¸å‰ 10 åï¼‰  # // NEW
    def top_predicted_classes_on_forget(self, model, topk=10):
        from collections import Counter
        model.eval()
        ctr = Counter()
        with torch.no_grad():
            for x, _ in self.forget_test_loader:
                x = x.to(self.device)
                pred = model(x).argmax(dim=1).cpu().tolist()
                ctr.update(pred)
        total = sum(ctr.values())
        top = ctr.most_common(topk)
        print("[Forget Top Pred Classes] (class_id, count, ratio)")
        for cid, cnt in top:
            print(f"  {cid:3d}  {cnt:6d}  {cnt/max(total,1):.3f}")
        return ctr

    def plot_forget_distributions(self, model, tag: str, out_dir: str, exp_id: str = None):
        """
        å° forget test set ç•« MSP / Entropy / Logit L2 ç¯„æ•¸ çš„ç›´æ–¹åœ–
        å­˜æˆä¸‰å¼µåœ–ï¼š{tag}_msp.png, {tag}_entropy.png, {tag}_logit_norm.png
        """
        if len(self.forget_test_loader.dataset) == 0:
            return None
        model.eval()
        msp_list=[]; ent_list=[]; norm_list=[]

        with torch.no_grad():
            for x, _ in self.forget_test_loader:
                x = x.to(self.device)
                logits = model(x)
                probs = F.softmax(logits, dim=1)
                msp = probs.max(dim=1).values
                ent = -(probs * (probs + 1e-12).log()).sum(dim=1)
                ln = logits.norm(p=2, dim=1)

                msp_list.append(msp.cpu()); ent_list.append(ent.cpu()); norm_list.append(ln.cpu())

        os.makedirs(out_dir, exist_ok=True)

        def _save_hist(arr, title, fname, xlabel=None):
            arr = torch.cat(arr).numpy()
            plt.figure(figsize=(5,4))
            plt.hist(arr, bins=40)
            plt.title(title) 
            plt.xlabel(xlabel)
            plt.ylabel("count (samples)")
            plt.tight_layout()

            final_fname = f"{exp_id}_{fname}" if exp_id else fname
            path = os.path.join(out_dir, final_fname)
            plt.savefig(path); plt.close()
            print(f"[Distributions] saved: {path}")

        _save_hist(msp_list, f"{tag} MSP ({exp_id or ''})", f"{tag}_msp.png", xlabel="max softmax probability")
        _save_hist(ent_list, f"{tag} Entropy ({exp_id or ''})", f"{tag}_entropy.png", xlabel="entropy (âˆ’âˆ‘p log p)")
        _save_hist(norm_list, f"{tag} Logit L2 Norm ({exp_id or ''})", f"{tag}_logit_norm.png", xlabel="â€–logitsâ€–â‚‚")

    def calculate_metrics(self):
        """è¨ˆç®—æ©Ÿå™¨éºå¿˜æŒ‡æ¨™"""
        if not self.results['original_metrics'] or not self.results['unlearned_metrics']:
            print("è«‹å…ˆè©•ä¼°æ¨¡å‹æ€§èƒ½")
            return
        
        # æå–æŒ‡æ¨™
        orig_retain = self.results['original_metrics']['retain_acc']
        orig_forget = self.results['original_metrics']['forget_acc']
        unl_retain = self.results['unlearned_metrics']['retain_acc']
        unl_forget = self.results['unlearned_metrics']['forget_acc'] or 0
        
        # è¨ˆç®—éºå¿˜æ•ˆæœ (1 - éºå¿˜é›†æº–ç¢ºç‡ä¸‹é™æ¯”ä¾‹)
        if orig_forget > 0:
            forget_effect = (orig_forget - unl_forget) / orig_forget
        else:
            forget_effect = 1.0
        
        # è¨ˆç®—ä¿ç•™æ•ˆæœ (ä¿ç•™é›†æº–ç¢ºç‡ç›¸å°è®ŠåŒ–)
        retain_effect = (unl_retain - orig_retain) / orig_retain
        
        # è¨ˆç®—ç¶œåˆæŒ‡æ¨™
        if 'retrained_metrics' in self.results and self.results['retrained_metrics']:
            gs_retain = self.results['retrained_metrics']['retain_acc']
            
            # GSè¿‘ä¼¼åº¦ - èˆ‡é»ƒé‡‘æ¨™æº–çš„æ¥è¿‘ç¨‹åº¦
            gs_similarity = unl_retain / gs_retain if gs_retain > 0 else 0
            
            self.results['gs_comparison'] = {
                'gs_similarity': gs_similarity,
                'unl_vs_gs_diff': unl_retain - gs_retain
            }
        
        # ä¿å­˜çµæœ
        self.results['forget_effect'] = forget_effect
        self.results['retain_effect'] = retain_effect
        self.results['MU_score'] = forget_effect * (1 + retain_effect)  # ç¶œåˆæ©Ÿå™¨éºå¿˜åˆ†æ•¸
        
        print("\n===== æ©Ÿå™¨éºå¿˜æŒ‡æ¨™ =====")
        print(f"éºå¿˜æ•ˆæœ: {forget_effect:.4f} (1.0 = å®Œå…¨éºå¿˜)")
        print(f"ä¿ç•™æ•ˆæœ: {retain_effect:.4f} (>0 = çŸ¥è­˜ä¿ç•™å¾—æ›´å¥½)")
        print(f"MUç¶œåˆåˆ†æ•¸: {self.results['MU_score']:.4f} (è¶Šé«˜è¶Šå¥½)")
        
        if 'gs_comparison' in self.results:
            print(f"é»ƒé‡‘æ¨™æº–è¿‘ä¼¼åº¦: {self.results['gs_comparison']['gs_similarity']:.4f}")
            print(f"èˆ‡é»ƒé‡‘æ¨™æº–å·®ç•°: {self.results['gs_comparison']['unl_vs_gs_diff']:.2f}%")
        
        return self.results

    def save_complete_report(self, filepath=None):
        """
        ä¿å­˜è©³ç´°çš„å¯¦é©—å ±å‘Šï¼ŒåŒ…æ‹¬æ‰€æœ‰åƒæ•¸è¨­å®šå’Œçµæœ
        
        Args:
            filepath: å ±å‘Šå­˜å„²è·¯å¾‘ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é»˜èªè·¯å¾‘
        """
        if filepath is None:
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            filepath = f"{self.output_dir}/unlearning_report_{timestamp}.txt"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            # 1. å ±å‘Šæ¨™é¡Œ
            f.write("="*80 + "\n")
            f.write(f"æ©Ÿå™¨éºå¿˜(Machine Unlearning)å¯¦é©—å ±å‘Š\n")
            f.write(f"æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            
            # 2. éºå¿˜è¨­å®š
            f.write("-"*80 + "\n")
            f.write("éºå¿˜è¨­å®š\n")
            f.write("-"*80 + "\n")
            if self.forget_classes is not None:
                f.write(f"éºå¿˜é¡åˆ¥: {sorted(self.forget_classes)}\n")
                f.write(f"ä¿ç•™é¡åˆ¥æ•¸: {self.num_retain_classes}\n")
                f.write(f"ç¸½é¡åˆ¥æ•¸: {self.total_classes}\n")
            else:
                f.write(f"éºå¿˜æ¨£æœ¬æ•¸: {len(self.forget_train_set)}\n")
                f.write(f"ä¿ç•™æ¨£æœ¬æ•¸: {len(self.retain_train_set)}\n")
            f.write("\n")
            
            # 3. æ¨¡å‹æ¶æ§‹
            f.write("-"*80 + "\n")
            f.write("æ¨¡å‹æ¶æ§‹åƒæ•¸\n")
            f.write("-"*80 + "\n")
            for k, v in self.model_args.items():
                f.write(f"{k}: {v}\n")
            f.write("\n")
            
            # 4. è¨“ç·´è¨­å®š
            f.write("-"*80 + "\n")
            f.write("è¨“ç·´è¨­å®š\n")
            f.write("-"*80 + "\n")
            f.write(f"è¨­å‚™: {self.device}\n")
            f.write(f"éºå¿˜æ™‚é–“: {self.results['unlearning_time']:.2f} ç§’\n")
            if 'retraining_time' in self.results:
                f.write(f"é»ƒé‡‘æ¨™æº–é‡è¨“ç·´æ™‚é–“: {self.results['retraining_time']:.2f} ç§’\n")
            f.write("\n")
            
            # 5. æ€§èƒ½æŒ‡æ¨™
            f.write("-"*80 + "\n")
            f.write("æ¨¡å‹æ€§èƒ½å°æ¯”\n")
            f.write("-"*80 + "\n")
            f.write("åŸå§‹æ¨¡å‹:\n")
            f.write(f"  ä¿ç•™é›†æº–ç¢ºç‡: {self.results['original_metrics'].get('retain_acc', 0):.2f}%\n")
            f.write(f"  éºå¿˜é›†æº–ç¢ºç‡: {self.results['original_metrics'].get('forget_acc', 0):.2f}%\n\n")
            
            f.write("éºå¿˜å¾Œæ¨¡å‹:\n")
            f.write(f"  ä¿ç•™é›†æº–ç¢ºç‡: {self.results['unlearned_metrics'].get('retain_acc', 0):.2f}%\n")
            forget_acc = self.results['unlearned_metrics'].get('forget_acc', None)
            if forget_acc is not None:
                f.write(f"  éºå¿˜é›†æº–ç¢ºç‡: {forget_acc:.2f}%\n\n")
            else:
                f.write("  éºå¿˜é›†æº–ç¢ºç‡: N/A (å·²ç§»é™¤é¡åˆ¥)\n\n")
            
            if 'retrained_metrics' in self.results:
                f.write("é»ƒé‡‘æ¨™æº–æ¨¡å‹:\n")
                f.write(f"  ä¿ç•™é›†æº–ç¢ºç‡: {self.results['retrained_metrics'].get('retain_acc', 0):.2f}%\n")
                gs_forget_acc = self.results['retrained_metrics'].get('forget_acc', None)
                if gs_forget_acc is not None:
                    f.write(f"  éºå¿˜é›†æº–ç¢ºç‡: {gs_forget_acc:.2f}%\n\n")
                else:
                    f.write("  éºå¿˜é›†æº–ç¢ºç‡: N/A (å·²ç§»é™¤é¡åˆ¥)\n\n")
            
            # 6. éºå¿˜æŒ‡æ¨™
            f.write("-"*80 + "\n")
            f.write("æ©Ÿå™¨éºå¿˜æŒ‡æ¨™\n")
            f.write("-"*80 + "\n")
            f.write(f"éºå¿˜æ•ˆæœ: {self.results.get('forget_effect', 0):.4f}\n")
            f.write(f"ä¿ç•™æ•ˆæœ: {self.results.get('retain_effect', 0):.4f}\n")
            f.write(f"MUç¶œåˆåˆ†æ•¸: {self.results.get('MU_score', 0):.4f}\n")
            
            if 'gs_comparison' in self.results:
                f.write(f"é»ƒé‡‘æ¨™æº–è¿‘ä¼¼åº¦: {self.results['gs_comparison'].get('gs_similarity', 0):.4f}\n")
                f.write(f"èˆ‡é»ƒé‡‘æ¨™æº–å·®ç•°: {self.results['gs_comparison'].get('unl_vs_gs_diff', 0):.2f}%\n")
            f.write("\n")
            
            # 7. MIAçµæœ
            if 'mia_results' in self.results and self.results['mia_results']:
                f.write("-"*80 + "\n")
                f.write("æˆå“¡æ¨æ–·æ”»æ“Š(MIA)çµæœ\n")
                f.write("-"*80 + "\n")
                f.write(f"ç¸½é«”æ”»æ“Šæº–ç¢ºç‡: {self.results['mia_results'].get('accuracy', 0):.4f}\n")
                f.write(f"ä¿ç•™é›†è­˜åˆ¥ç‡: {self.results['mia_results'].get('retain_acc', 0):.4f}\n")
                f.write(f"éºå¿˜é›†è­˜åˆ¥ç‡: {self.results['mia_results'].get('forget_acc', 0):.4f}\n")
                f.write(f"MIAéºå¿˜æ•ˆæœè©•åˆ†: {self.results['mia_results'].get('forget_effect', 0):.4f}\n\n")
                
                # æ·»åŠ MIAåˆ†æçµæœ
                f.write("MIAåˆ†æ:\n")
                if self.results['mia_results'].get('forget_acc', 0) <= 0.55:
                    f.write("  éºå¿˜é›†è­˜åˆ¥ç‡æ¥è¿‘éš¨æ©ŸçŒœæ¸¬æ°´å¹³ï¼Œè¡¨æ˜éºå¿˜éå¸¸æˆåŠŸã€‚\n")
                else:
                    f.write("  éºå¿˜é›†è­˜åˆ¥ç‡é«˜æ–¼éš¨æ©ŸçŒœæ¸¬ï¼Œè¡¨æ˜éºå¿˜å¯èƒ½ä¸å®Œå…¨ã€‚\n")
                
                if self.results['mia_results'].get('retain_acc', 0) >= 0.9:
                    f.write("  ä¿ç•™é›†è­˜åˆ¥ç‡å¾ˆé«˜ï¼Œè¡¨æ˜æ¨¡å‹å°ä¿ç•™æ•¸æ“šçš„è¨˜æ†¶å¾ˆå¼·ï¼Œä½†é€™å¯èƒ½å°è‡´éš±ç§é¢¨éšªã€‚\n")
                f.write("\n")
            
            # 8. çµè«–èˆ‡å»ºè­°
            f.write("-"*80 + "\n")
            f.write("çµè«–èˆ‡å»ºè­°\n")
            f.write("-"*80 + "\n")
            
            # æ ¹æ“šçµæœç”Ÿæˆçµè«–
            forget_effect = self.results.get('forget_effect', 0)
            retain_effect = self.results.get('retain_effect', 0)
            mu_score = self.results.get('MU_score', 0)
            
            if forget_effect > 0.95 and retain_effect >= 0:
                f.write("å¯¦é©—çµæœè¡¨æ˜ï¼Œæ‰€æ¡ç”¨çš„éºå¿˜æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œå®Œå…¨ç§»é™¤äº†ç›®æ¨™çŸ¥è­˜ï¼ŒåŒæ™‚ä¿æŒäº†æ¨¡å‹åœ¨ä¿ç•™æ•¸æ“šä¸Šçš„æ€§èƒ½ã€‚\n")
            elif forget_effect > 0.8:
                f.write("å¯¦é©—çµæœè¡¨æ˜ï¼Œæ‰€æ¡ç”¨çš„éºå¿˜æ–¹æ³•æ•ˆæœè‰¯å¥½ï¼Œå¤§éƒ¨åˆ†ç§»é™¤äº†ç›®æ¨™çŸ¥è­˜ã€‚\n")
            else:
                f.write("å¯¦é©—çµæœè¡¨æ˜ï¼Œæ‰€æ¡ç”¨çš„éºå¿˜æ–¹æ³•æ•ˆæœæœ‰é™ï¼Œæœªèƒ½å®Œå…¨ç§»é™¤ç›®æ¨™çŸ¥è­˜ã€‚å»ºè­°å˜—è©¦å…¶ä»–éºå¿˜ç­–ç•¥ã€‚\n")
            
            if 'gs_comparison' in self.results:
                gs_similarity = self.results['gs_comparison'].get('gs_similarity', 0)
                if gs_similarity > 1.2:
                    f.write("å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéºå¿˜æ¨¡å‹æ€§èƒ½æ˜é¡¯å„ªæ–¼é»ƒé‡‘æ¨™æº–æ¨¡å‹ï¼Œé€™å¯èƒ½è¡¨æ˜é»ƒé‡‘æ¨™æº–æ¨¡å‹è¨“ç·´ä¸è¶³ã€‚å»ºè­°å¢åŠ é»ƒé‡‘æ¨™æº–æ¨¡å‹çš„è¨“ç·´è¼ªæ•¸æˆ–èª¿æ•´å­¸ç¿’ç‡ã€‚\n")
            
            if 'mia_results' in self.results:
                mia_forget_effect = self.results['mia_results'].get('forget_effect', 0)
                if mia_forget_effect < 0.7:
                    f.write("é›–ç„¶å¾ä»»å‹™æ€§èƒ½ä¾†çœ‹éºå¿˜æ•ˆæœè‰¯å¥½ï¼Œä½†MIAè©•ä¼°è¡¨æ˜éš±ç§ä¿è­·ç¨‹åº¦æœ‰é™ã€‚å»ºè­°è€ƒæ…®çµåˆå·®åˆ†éš±ç§æŠ€è¡“ä»¥å¢å¼·éš±ç§ä¿è­·ã€‚\n")
        
        print(f"è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {filepath}")
        return filepath

    def visualize_results(self):
        # ç”¢ç”Ÿä¸€å¼µåŸºæœ¬çš„æŸ±ç‹€åœ–ï¼šåŸå§‹/éºå¿˜/GS çš„ä¿ç•™é›† Acc
        try:
            names, accs = [], []
            if self.results.get('original_metrics'):
                names.append('Original'); accs.append(self.results['original_metrics'].get('retain_acc', 0))
            if self.results.get('unlearned_metrics'):
                names.append('Unlearned'); accs.append(self.results['unlearned_metrics'].get('retain_acc', 0))
            if self.results.get('retrained_metrics'):
                names.append('Gold'); accs.append(self.results['retrained_metrics'].get('retain_acc', 0))
            if names:
                plt.figure(figsize=(5,4))
                plt.bar(names, accs)
                plt.ylabel('Retain Test Acc (%)')
                plt.title('Retain Accuracy Comparison')
                out = os.path.join(self.output_dir, 'retain_acc_comparison.png')
                plt.tight_layout(); plt.savefig(out); plt.close()
                print(f"å·²è¼¸å‡ºåœ–æª”ï¼š{out}")
        except Exception as e:
            print(f"visualize_results ç™¼ç”Ÿä¾‹å¤–ï¼š{e}")


class MembershipInferenceAttack:
    """æˆå“¡æ¨æ–·æ”»æ“Š(MIA)å¯¦ç¾ - è©•ä¼°æ¨¡å‹æ˜¯å¦çœŸæ­£"éºå¿˜"äº†ç›®æ¨™æ•¸æ“š"""
    
    def __init__(self, target_model, device='cuda'):
        """
        åˆå§‹åŒ–MIAè©•ä¼°å™¨
        
        Args:
            target_model: ç›®æ¨™æ¨¡å‹ (éºå¿˜å¾Œçš„æ¨¡å‹)
            device: è¨ˆç®—è¨­å‚™
        """
        self.target_model = target_model
        self.device = device
        self.attack_model = None
    
    def extract_features(self, model, loader, temperature: float = 3.0):
        """
        å¾ç›®æ¨™æ¨¡å‹ä¸­æå– MIA ç‰¹å¾µ:
        - MSP (æœ€å¤§ softmax æ©Ÿç‡, ç¶“éæº«åº¦ç¸®æ”¾)
        - Entropy (æ©Ÿç‡åˆ†å¸ƒç†µ, ç¶“éæº«åº¦ç¸®æ”¾)
        - Margin (top1 - top2 æ©Ÿç‡å·®)
        - Per-sample CE Loss
        """
        model.eval()
        all_feats, all_labels = [], []
        ce = torch.nn.CrossEntropyLoss(reduction='none')

        with torch.no_grad():
            for x, y in loader:
                x, y = x.to(self.device), y.to(self.device)

                # åŸå§‹ logits
                logits = model(x)
                
                # Temperature scaling
                logits_T = logits / temperature
                probs_T = torch.softmax(logits_T, dim=1)

                # åŸºæœ¬ç‰¹å¾µ
                msp = probs_T.max(dim=1).values  # æœ€å¤§æ©Ÿç‡
                entropy = -(probs_T * probs_T.clamp_min(1e-12).log()).sum(dim=1)
                loss = ce(logits, y)             # æ³¨æ„: ç”¨åŸå§‹ logits ç®— CE
                top2 = torch.topk(probs_T, k=2, dim=1).values
                margin = top2[:, 0] - top2[:, 1]

                # å †æˆ feature vector: [msp, entropy, margin, loss]
                feats = torch.stack([msp, entropy, margin, loss], dim=1)

                all_feats.append(feats.cpu())
                all_labels.append(y.cpu())

        X = torch.cat(all_feats, dim=0).numpy()
        y = torch.cat(all_labels, dim=0).numpy()

        return X, y
    
    def prepare_attack_data(self, retain_loader, forget_loader, test_loader):
        """
        æº–å‚™æˆå“¡æ¨æ–·æ”»æ“Šæ‰€éœ€çš„æ•¸æ“š
        
        Args:
            retain_loader: ä¿ç•™æ•¸æ“šåŠ è¼‰å™¨
            forget_loader: éºå¿˜æ•¸æ“šåŠ è¼‰å™¨
            test_loader: æ¸¬è©¦æ•¸æ“šåŠ è¼‰å™¨
        
        Returns:
            X_train, y_train, X_test, y_test, original_labels_test: è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“šé›†
        """
        print("æå–ä¿ç•™é›†ç‰¹å¾µ...")
        retain_X, retain_y = self.extract_features(self.target_model, retain_loader)

        print("æå–éºå¿˜é›†ç‰¹å¾µ...")
        forget_X, forget_y = self.extract_features(self.target_model, forget_loader)

        print("æå–æ¸¬è©¦é›†ç‰¹å¾µ...")
        test_X, test_y = self.extract_features(self.target_model, test_loader)

        # 2) æˆå“¡æ¨™ç±¤ï¼ˆæ”»æ“Šç›®æ¨™ï¼‰ï¼šretain=1, forget=0, test=0
        retain_m = np.ones(len(retain_X),  dtype=np.int32)          # æˆå“¡
        forget_m = np.zeros(len(forget_X), dtype=np.int32)          # éæˆå“¡ï¼ˆç†æ‡‰è¢«å¿˜ï¼‰
        test_m   = np.zeros(len(test_X),   dtype=np.int32)          # éæˆå“¡

        # ğŸ“‹ æˆå“¡èº«ä»½æ¨™ç±¤åˆ†å¸ƒ
        total_members = len(retain_X)
        total_non_members = len(forget_X) + len(test_X)
        total_samples = total_members + total_non_members

        print(f"\nğŸ·ï¸  æˆå“¡èº«ä»½æ¨™ç±¤åˆ†å¸ƒ:")
        print(f"æˆå“¡ (æ¨™ç±¤=1): {total_members:,} å€‹ ({total_members/total_samples*100:.1f}%)")
        print(f"éæˆå“¡ (æ¨™ç±¤=0): {total_non_members:,} å€‹ ({total_non_members/total_samples*100:.1f}%)")
        print(f"æˆå“¡:éæˆå“¡æ¯”ä¾‹ = 1:{total_non_members/total_members:.2f}")
        
        # ğŸ“ˆ å„çµ„è©³ç´°åˆ†è§£
        print(f"\nğŸ“ˆ å„çµ„è©³ç´°åˆ†è§£:")
        print(f"  ä¿ç•™é›† (æˆå“¡):     {len(retain_X):,} å€‹ â†’ æ¨™ç±¤=1")
        print(f"  éºå¿˜é›† (éæˆå“¡):   {len(forget_X):,} å€‹ â†’ æ¨™ç±¤=0")
        print(f"  æ¸¬è©¦é›† (éæˆå“¡):   {len(test_X):,} å€‹ â†’ æ¨™ç±¤=0")


        # 2) ä¾†æºæ——æ¨™ï¼ˆåƒ…ç”¨æ–¼è©•ä¼°åˆ†ç¾¤ï¼‰ï¼š
        #    ä¿ç•™/éºå¿˜ä¿ç•™åŸå§‹é¡åˆ¥(>=0)ï¼›æ¸¬è©¦é›†æ¨™ç‚ºè² å€¼ä»¥ä¾¿åœ¨ evaluate æ™‚ç”¨ <0 ç¯©é¸
        retain_flag = retain_y.astype(np.int32)                 # >= 0
        forget_flag = forget_y.astype(np.int32)                 # >= 0
        test_flag   = (-test_y.astype(np.int32)) - 1

        # 4) â”€â”€ è¨“ç·´é›†ï¼šåªç”¨ retain(=1) + test(=0) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        n_retain = int(len(retain_X) * 0.8)
        n_test   = int(len(test_X) * 0.8)
        X_train = np.vstack([retain_X[:n_retain], test_X[:n_test]])
        y_train = np.concatenate([retain_m[:n_retain], test_m[:n_test]]).astype(np.int32)
        
        # 5) â”€â”€ æ¸¬è©¦é›†ï¼šåŒ…å« retain + forget + testï¼ˆæŒ‰æ­¤é †åºä¸²æ¥ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        X_test = np.vstack([retain_X[n_retain:], forget_X, test_X[n_test:]])
        y_test = np.concatenate([retain_m[n_retain:], forget_m, test_m[n_test:]])
        original_labels_test = np.concatenate([retain_flag[n_retain:], forget_flag, test_flag[n_test:]])

        # 6) è¨Šæ¯åˆ—å°ï¼ˆæ–¹ä¾¿æª¢æŸ¥è³‡æ–™æ¯”ä¾‹èˆ‡èªæ„ï¼‰
        total_members = int(retain_m.sum())
        total_nonmembers = int((forget_m.sum() + test_m.sum()))
        total = total_members + total_nonmembers
        print("\nğŸ·ï¸  æˆå“¡èº«ä»½æ¨™ç±¤åˆ†å¸ƒï¼ˆæ•´é«”èªæ„ï¼‰")
        print(f"  æˆå“¡(=1):   {total_members:,} / {total:,} ({total_members/total*100:.1f}%)")
        print(f"  éæˆå“¡(=0): {total_nonmembers:,} / {total:,} ({total_nonmembers/total*100:.1f}%)")
        print(f"  è¨“ç·´é›†ï¼šretain(1)={len(retain_X):,}, test(0)={len(test_X):,}  â†’ X_train={len(X_train):,}")
        print(f"  æ¸¬è©¦é›†ï¼šretain={len(retain_X):,}, forget={len(forget_X):,}, test={len(test_X):,} â†’ X_test={len(X_test):,}")
        
        self.original_labels_test = original_labels_test

        return X_train, y_train, X_test, y_test, original_labels_test

    def train_attack_model(self, X_train, y_train, X_val, y_val,
                        epochs=50, learning_rate=1e-3, use_scheduler=True):
        """
        è¨“ç·´ MIA æ”»æ“Šæ¨¡å‹ï¼ˆé©—è­‰ä»¥ AUC ç‚ºä¸»ï¼›ä¸ä½¿ç”¨å›ºå®š0.5ï¼‰
        X_val/y_val å»ºè­°æ˜¯ retain(1)+test(0) çš„é©—è­‰åˆ‡åˆ†ï¼›forget åƒ…ä½œæœ€çµ‚ evaluate_attack
        """
        # ---- Dataset / Loader ----
        train_ds = torch.utils.data.TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.float32)
        )
        val_ds = torch.utils.data.TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_val, dtype=torch.float32)
        )
        train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)
        val_loader = torch.utils.data.DataLoader(val_ds, batch_size=256, shuffle=False)

        # ---- æ”»æ“Šå™¨ï¼ˆæœ€å¾Œè¼¸å‡º logitsï¼›ä¸è¦ Sigmoidï¼‰----
        class AttackModel(nn.Module):
            def __init__(self, input_dim):
                super().__init__()
                self.net = nn.Sequential(
                    nn.Linear(input_dim, 64),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.BatchNorm1d(64),
                    nn.Linear(64, 32),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.BatchNorm1d(32),
                    nn.Linear(32, 16),
                    nn.ReLU(),
                    nn.Linear(16, 1)  # logits
                )
            def forward(self, x):
                return self.net(x).view(-1)

        input_dim = X_train.shape[1]
        self.attack_model = AttackModel(input_dim).to(self.device)

        optimizer = optim.Adam(self.attack_model.parameters(), lr=learning_rate, weight_decay=1e-4)

        # ---- ä¸å¹³è¡¡å‹å–„çš„ Loss ----
        pos = max(1, int((y_train == 1).sum()))
        neg = max(1, int((y_train == 0).sum()))
        pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=self.device)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        if use_scheduler:
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-5, verbose=True
            )
        else:
            scheduler = None

        best_auc = -1.0
        best_state = None
        best_thr = 0.5  # åªä½œç´€éŒ„ç”¨

        for epoch in range(epochs):
            # ---- Train ----
            self.attack_model.train()
            running = 0.0
            for xb, yb in train_loader:
                xb, yb = xb.to(self.device), yb.to(self.device)
                optimizer.zero_grad()
                logits = self.attack_model(xb)               # logits
                loss = criterion(logits, yb)                 # BCEWithLogitsLoss
                loss.backward()
                optimizer.step()
                running += loss.item() * len(xb)
            train_loss = running / len(train_loader.dataset)

            # ---- Validate: ä»¥ AUC ç‚ºä¸»ï¼›åŒæ™‚è¨ˆç®—æœ€ä½³é–¾å€¼çš„ Accï¼ˆåªç‚ºåƒè€ƒï¼Œä¸å›å‚³0.5ï¼‰----
            self.attack_model.eval()
            all_logits, all_labels = [], []
            with torch.no_grad():
                for xb, yb in val_loader:
                    xb = xb.to(self.device)
                    lg = self.attack_model(xb)               # logits
                    all_logits.append(lg.cpu())
                    all_labels.append(yb)
            all_logits = torch.cat(all_logits).numpy()
            all_labels = torch.cat(all_labels).numpy()
            probs = torch.sigmoid(torch.tensor(all_logits)).numpy()

            try:
                val_auc = roc_auc_score(all_labels, probs)
            except ValueError:
                # y_val è‹¥ä¸å«å…©é¡ï¼ŒAUC ç„¡æ³•è¨ˆç®—ï¼›é€€å› 0.5
                val_auc = 0.5

            # æ‰¾æœ€ä½³é–¾å€¼ï¼ˆYouden's Jï¼‰åƒ…ä½œ log åƒè€ƒ
            try:
                fpr, tpr, th = roc_curve(all_labels, probs)
                j = (tpr - fpr)
                j_idx = int(j.argmax())
                thr = float(th[j_idx])
                y_bin = (probs >= thr).astype(int)
                val_acc_best = accuracy_score(all_labels, y_bin)
            except Exception:
                thr, val_acc_best = 0.5, 0.0

            if scheduler is not None:
                scheduler.step(val_auc)

            print(f"[Epoch {epoch+1:03d}] TrainLoss={train_loss:.4f}  ValAUC={val_auc:.4f}  "
                f"(best_thr~{thr:.3f}, Acc@best={val_acc_best:.4f})  LR={optimizer.param_groups[0]['lr']:.6f}")

            if val_auc > best_auc:
                best_auc = val_auc
                best_thr = thr
                best_state = copy.deepcopy(self.attack_model.state_dict())

        if best_state is not None:
            self.attack_model.load_state_dict(best_state)
            self.best_attack_threshold = best_thr  # å¯ä¾› evaluate_attack ä½¿ç”¨æˆ–åˆ—å°
            print(f"å·²æ¢å¾©æœ€ä½³æ”»æ“Šå™¨ (Val AUC={best_auc:.4f}, best_thr={best_thr:.3f})")

        return self.attack_model
    
    def evaluate_attack(self, X_test, y_test, original_labels_test):
        """
        åœ¨ retain / forget / test ä¸Šè©•ä¼°æ”»æ“Šå™¨æ•ˆæœ
        Args:
            X_test: æ¸¬è©¦é›†ç‰¹å¾µ (numpy array, [N, d])
            y_test: membership æ¨™ç±¤ (1=retain, 0=forget/test)
            original_labels_test: ä¾†æºæ——æ¨™ (>=0: retain/forget, <0: test)
        Returns:
            dict åŒ…å«ç¸½é«”èˆ‡åˆ†çµ„æŒ‡æ¨™
        """
        self.attack_model.eval()
        with torch.no_grad():
            X_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)
            logits = self.attack_model(X_tensor)          # shape [N] æˆ– [N,1]
            if logits.ndim > 1:
                logits = logits.view(-1)                  # å¼·åˆ¶ [N]
            probs = torch.sigmoid(logits).cpu().numpy()   # è½‰æˆæ©Ÿç‡ [N]

        # ---- ä¿è­‰ y_test èˆ‡ probs å°é½Š ----
        y_test = np.array(y_test).ravel()
        probs = np.array(probs).ravel()
        assert probs.shape[0] == y_test.shape[0], \
            f"Shape mismatch: probs={probs.shape}, y_test={y_test.shape}"

        # --- ç¸½é«” AUC ---
        auc = roc_auc_score(y_test, probs)

        # --- æ‰¾æœ€ä½³é–¾å€¼ (Youdenâ€™s J index) ---
        fpr, tpr, thr = roc_curve(y_test, probs)
        youden_idx = np.argmax(tpr - fpr)
        best_thr = thr[youden_idx]

        # --- ç¸½é«” Accuracy @æœ€ä½³é–¾å€¼ ---
        y_pred = (probs >= best_thr).astype(int)
        acc = accuracy_score(y_test, y_pred)

        results = {
            "AUC": float(auc),
            "best_threshold": float(best_thr),
            "overall_acc": float(acc),
        }

        # --- åˆ†çµ„æŒ‡æ¨™ ---
        group_metrics = {}
        groups = {
            "retain": (y_test == 1),
            "forget": ((y_test == 0) & (original_labels_test >= 0)),
            "test":   ((y_test == 0) & (original_labels_test < 0)),
        }

        for name, idx in groups.items():
            if idx.any():
                grp_acc = accuracy_score(y_test[idx], y_pred[idx])
                group_metrics[name] = {
                    "size": int(idx.sum()),
                    "acc": float(grp_acc)
                }

        results["groups"] = group_metrics

        print("\nğŸ“Š MIA è©•ä¼°çµæœ")
        print(f"  AUC = {auc:.4f}, best_thr = {best_thr:.4f}, Overall Acc = {acc:.4f}")
        for g, m in group_metrics.items():
            print(f"  {g:>6}: size={m['size']}, acc={m['acc']:.4f}")

        return results

    def plot_feature_distribution(self, features, labels, original_labels_test, title="MIA Feature Distribution", output_dir=None):
        """
        ç•«å‡º retain / forget / test åœ¨ä¸åŒç‰¹å¾µä¸Šçš„åˆ†å¸ƒ
        features: [N, d]ï¼Œç›®å‰ d=4 (MSP, Entropy, Margin, Loss)
        labels: membership æ¨™ç±¤ (1=retain, 0=forget/test)
        """
        self.original_labels_test = original_labels_test

        retain_idx = (labels == 1)
        forget_idx = (labels == 0) & (self.original_labels_test >= 0)
        test_idx   = (labels == 0) & (self.original_labels_test < 0)

        names = ["MSP (confidence)", "Entropy", "Margin", "Loss"]
        num_feats = features.shape[1]

        plt.figure(figsize=(14, 10))
        for i in range(num_feats):
            plt.subplot((num_feats+1)//2, 2, i+1)
            if retain_idx.any():
                plt.hist(features[retain_idx, i], bins=40, alpha=0.5, label="Retain")
            if forget_idx.any():
                plt.hist(features[forget_idx, i], bins=40, alpha=0.5, label="Forget")
            if test_idx.any():
                plt.hist(features[test_idx, i], bins=40, alpha=0.5, label="Test")
            plt.title(names[i] if i < len(names) else f"Feature {i}")
            plt.legend()

        plt.suptitle(title)
        plt.tight_layout()

        if output_dir:
            save_path = os.path.join(output_dir, f"{title.replace(' ','_')}.png")
            plt.savefig(save_path)
            print(f"Feature distribution saved to {save_path}")
        else:
            plt.show()

def main():
    # å‘½ä»¤è¡Œåƒæ•¸è¨­ç½®
    parser = argparse.ArgumentParser(description='æ©Ÿå™¨éºå¿˜(Machine Unlearning)å¯¦é©—')
    parser.add_argument("--num_epochs", type=int, default=80, help="éºå¿˜è¨“ç·´çš„è¼ªæ•¸")
    parser.add_argument("--gs_epochs", type=int, default=250, help="é»ƒé‡‘æ¨™æº–æ¨¡å‹è¨“ç·´çš„è¼ªæ•¸")
    parser.add_argument("--batch_size", type=int, default=128, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="éºå¿˜è¨“ç·´çš„åˆå§‹å­¸ç¿’ç‡")
    parser.add_argument("--lr_min", type=float, default=1e-6, help="éºå¿˜è¨“ç·´çš„æœ€å°å­¸ç¿’ç‡")
    parser.add_argument("--lr_scheduler", type=str, choices=['cosine', 'step', 'plateau', 'onecycle'], 
                       default='cosine', help="å­¸ç¿’ç‡èª¿åº¦å™¨é¡å‹")
    parser.add_argument("--gs_learning_rate", type=float, default=1e-3, help="é»ƒé‡‘æ¨™æº–æ¨¡å‹çš„åˆå§‹å­¸ç¿’ç‡")
    parser.add_argument("--gs_lr_min", type=float, default=1e-6, help="é»ƒé‡‘æ¨™æº–æ¨¡å‹çš„æœ€å°å­¸ç¿’ç‡")
    parser.add_argument("--gs_lr_scheduler", type=str, choices=['cosine', 'onecycle', 'step'], 
                       default='onecycle', help="é»ƒé‡‘æ¨™æº–æ¨¡å‹çš„å­¸ç¿’ç‡èª¿åº¦å™¨é¡å‹")
    parser.add_argument("--mia_learning_rate", type=float, default=1e-3, help="MIAæ¨¡å‹çš„å­¸ç¿’ç‡")
    parser.add_argument("--mia_epochs", type=int, default=50, help="MIAè¨“ç·´è¼ªæ•¸")
    parser.add_argument("--mia_use_scheduler", type=bool, default=True, help="æ˜¯å¦å°MIAæ¨¡å‹ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨")
    parser.add_argument("--output_dir", type=str, default="./unlearning_output", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--best_model_path", type=str, 
                      default="/home/davidhuang/vits-for-small-scale-datasets/checkpoints/ViT_classattn_CIFAR100/BEST_ViT_20250423-0016_lr0.001_bs256_epochs600/best_vit_20250423-0016.pth", 
                      help="é è¨“ç·´æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--unlearning_method", type=str, 
                      choices=['head_retrain', 'negative_gradient'], 
                      default='head_retrain', 
                      help="éºå¿˜æ–¹æ³•é¸æ“‡")
    parser.add_argument("--forget_classes", type=str, default="20-29", 
                      help="è¦éºå¿˜çš„é¡åˆ¥ (e.g., '20-29' or '20,21,22')")
    parser.add_argument("--no_mia", action="store_false", dest="run_mia", 
                    default=True, help="è¨­ç½®æ­¤åƒæ•¸ç¦ç”¨MIAè©•ä¼°")
    args = parser.parse_args()
    
    # è™•ç†éºå¿˜é¡åˆ¥åƒæ•¸
    if '-' in args.forget_classes:
        start, end = map(int, args.forget_classes.split('-'))
        forget_classes = set(range(start, end+1))
    else:
        forget_classes = set(map(int, args.forget_classes.split(',')))
    
    # æ¨¡å‹åƒæ•¸
    model_args = {
        'img_size': 32,
        'patch_size': 4,
        'in_chans': 3,
        'num_classes': 100,
        'embed_dim': 300,
        'depth': 10,
        'num_heads': 12,
        'mlp_ratio': 4.0,
        'qkv_bias': True,
        'drop_rate': 0.0,
        'attn_drop_rate': 0.0,
        'drop_path_rate': 0.05,
        'is_LSA': True
    }
    
    # å‰µå»ºå¯¦é©—è¼¸å‡ºç›®éŒ„
    timestamp = time.strftime("%Y%m%d-%H%M")
    output_dir = f"{args.output_dir}/{args.unlearning_method}_forget{args.forget_classes}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å‘½ä»¤è¡Œåƒæ•¸
    with open(f"{output_dir}/args.txt", 'w') as f:
        for arg, value in vars(args).items():
            f.write(f"{arg}: {value}\n")
    
    # åˆå§‹åŒ–æ©Ÿå™¨éºå¿˜æ¡†æ¶
    mul = MachineUnlearning(
        model_class=VisionTransformer,
        model_args=model_args,
        device='cuda',
        output_dir=output_dir
    )
    
    # è¼‰å…¥é è¨“ç·´æ¨¡å‹
    mul.load_original_model(args.best_model_path)
    
    # æº–å‚™æ•¸æ“š
    mul.prepare_data(dataset_name='CIFAR100', forget_classes=forget_classes)
    
    # è©•ä¼°åŸå§‹æ¨¡å‹æ€§èƒ½
    print("\n1. è©•ä¼°åŸå§‹æ¨¡å‹")
    mul.evaluate_all_models()
    
    # é¸æ“‡éºå¿˜æ–¹æ³•
    print(f"\n2. åŸ·è¡Œæ©Ÿå™¨éºå¿˜ - {args.unlearning_method}")
    if args.unlearning_method == 'head_retrain':
        mul.unlearn_by_retraining_head(
            epochs=args.num_epochs, 
            lr=args.learning_rate, 
            lr_scheduler_type=args.lr_scheduler, 
            min_lr=args.lr_min
        )
    elif args.unlearning_method == 'negative_gradient':
        mul.unlearn_by_negative_gradient(
            epochs=args.num_epochs, 
            lr=args.learning_rate,
            retain_epochs=args.num_epochs//3,
            lr_scheduler_type=args.lr_scheduler,
            min_lr=args.lr_min
        )
    
    # è¨“ç·´é»ƒé‡‘æ¨™æº–
    print("\n3. è¨“ç·´é»ƒé‡‘æ¨™æº–æ¨¡å‹")
    mul.train_gold_standard(
        epochs=args.gs_epochs, 
        lr=args.gs_learning_rate,
        lr_scheduler_type=args.gs_lr_scheduler,
        min_lr=args.gs_lr_min
    )
    
    # è©•ä¼°æ‰€æœ‰æ¨¡å‹æ€§èƒ½
    print("\n4. è©•ä¼°æ‰€æœ‰æ¨¡å‹æ€§èƒ½")
    mul.evaluate_all_models()
    
    # è¨ˆç®—éºå¿˜æŒ‡æ¨™
    print("\n5. è¨ˆç®—æ©Ÿå™¨éºå¿˜æŒ‡æ¨™")
    mul.calculate_metrics()
    
    # é‹è¡ŒMIAæ”»æ“Šè©•ä¼°
    if args.run_mia:
        print("\n6. åŸ·è¡Œæˆå“¡æ¨æ–·æ”»æ“Š(MIA)è©•ä¼°")
        mul.run_mia(
            train_epochs=args.mia_epochs,
            learning_rate=args.mia_learning_rate,
            use_scheduler=args.mia_use_scheduler
        )
    
    # å¯è¦–åŒ–çµæœ
    print("\n7. å¯è¦–åŒ–çµæœ")
    mul.visualize_results()
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    print("\n8. ä¿å­˜è©³ç´°å ±å‘Š")
    mul.save_complete_report()
    
    # ä¿å­˜çµæœ
    print("\n9. ä¿å­˜çµæœ")
    mul.save_results()
    
    print("\næ©Ÿå™¨éºå¿˜å¯¦é©—å®Œæˆ!")
    print(f"æ‰€æœ‰çµæœä¿å­˜åœ¨ï¼š{output_dir}")


if __name__ == "__main__":
    main()