"""
MIA ç‰¹å¾µåˆ†å¸ƒè¦–è¦ºåŒ–å·¥å…·
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import warnings
import os
warnings.filterwarnings('ignore')

# è¨­ç½® matplotlib ä¸é¡¯ç¤ºåœ–å½¢
plt.ioff()
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def plot_feature_distributions(retain_features, forget_features, test_features, 
                             feature_names=None, save_path=None):
    """
    ç¹ªè£½å„çµ„ç‰¹å¾µåˆ†å¸ƒå°æ¯”åœ–
    """
    if feature_names is None:
        feature_names = ['confidence', 'entropy', 'margin', 'loss']
    
    n_features = retain_features.shape[1]
    feature_names = feature_names[:n_features]

    # ğŸ› ï¸ å‹•æ…‹è¨ˆç®—æœ€ä½³å¸ƒå±€
    if n_features <= 2:
        nrows, ncols = 1, n_features
    elif n_features <= 4:
        nrows, ncols = 2, 2  # 2x2 å¸ƒå±€
    elif n_features <= 6:
        nrows, ncols = 2, 3  # 2x3 å¸ƒå±€
    elif n_features <= 8:
        nrows, ncols = 2, 4  # 2x4 å¸ƒå±€
    else:
        nrows, ncols = 3, (n_features + 2) // 3  # 3è¡Œå¸ƒå±€

    # ğŸ› ï¸ å‰µå»ºé©ç•¶æ•¸é‡çš„å­åœ–
    fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 5*nrows))
    
    # ç¢ºä¿ axes æ˜¯æ•¸çµ„æ ¼å¼
    if n_features == 1:
        axes = [axes]
    elif nrows == 1 or ncols == 1:
        axes = axes.flatten()
    else:
        axes = axes.flatten()
    
    for i in range(n_features):
        ax = axes[i]
        
        # ç¹ªè£½åˆ†å¸ƒ
        ax.hist(retain_features[:, i], bins=50, alpha=0.6, density=True, 
               color='blue', label=f'Retain (Member)\n({len(retain_features):,} samples)')
        ax.hist(forget_features[:, i], bins=50, alpha=0.6, density=True, 
               color='red', label=f'Forget (Non-member)\n({len(forget_features):,} samples)')
        ax.hist(test_features[:, i], bins=30, alpha=0.4, density=True, 
               color='green', label=f'Test (Non-member)\n({len(test_features):,} samples)')
        
        # è¨ˆç®—çµ±è¨ˆé‡
        retain_mean = retain_features[:, i].mean()
        forget_mean = forget_features[:, i].mean()
        test_mean = test_features[:, i].mean()
        
        # æ·»åŠ å‡å€¼ç·š
        ax.axvline(retain_mean, color='blue', linestyle='--', linewidth=2)
        ax.axvline(forget_mean, color='red', linestyle='--', linewidth=2)
        ax.axvline(test_mean, color='green', linestyle='--', linewidth=2)
        
        # è¨ˆç®—é‡ç–Šåº¦
        overlap_retain_forget = calculate_overlap(retain_features[:, i], forget_features[:, i])
        overlap_retain_test = calculate_overlap(retain_features[:, i], test_features[:, i])
        
        ax.set_title(f'{feature_names[i]}\n'
                    f'R-F overlap: {overlap_retain_forget:.2f}, '
                    f'R-T overlap: {overlap_retain_test:.2f}')
        ax.set_xlabel('Feature Value')
        ax.set_ylabel('Density')
        ax.legend(fontsize=8)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ç‰¹å¾µåˆ†å¸ƒåœ–å·²ä¿å­˜: {save_path}")
    plt.close()
    
    # æ‰“å°çµ±è¨ˆæ‘˜è¦
    print_feature_statistics(retain_features, forget_features, test_features, feature_names)


def calculate_overlap(dist1, dist2):
    """è¨ˆç®—å…©å€‹åˆ†å¸ƒçš„é‡ç–Šåº¦"""
    # ä½¿ç”¨ç›´æ–¹åœ–ä¼°è¨ˆé‡ç–Š
    bins = np.linspace(min(min(dist1), min(dist2)), max(max(dist1), max(dist2)), 50)
    hist1, _ = np.histogram(dist1, bins=bins, density=True)
    hist2, _ = np.histogram(dist2, bins=bins, density=True)
    
    # è¨ˆç®—é‡ç–Šé¢ç©
    overlap = np.sum(np.minimum(hist1, hist2)) * (bins[1] - bins[0])
    return overlap


def plot_2d_feature_scatter(retain_features, forget_features, test_features, 
                           feature_names=None, save_path=None):
    """
    ç¹ªè£½é—œéµç‰¹å¾µçš„2Dæ•£é»åœ–
    """
    if feature_names is None:
        feature_names = ['confidence', 'entropy', 'variance', 'margin']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    # é—œéµç‰¹å¾µå°çµ„åˆ
    feature_pairs = [
        (0, 1),  # confidence vs entropy
        (0, 2),  # confidence vs variance  
        (0, 3),  # confidence vs margin
        (1, 2),  # entropy vs variance
        (1, 3),  # entropy vs margin
        (2, 3),  # variance vs margin
    ]
    
    for idx, (i, j) in enumerate(feature_pairs):
        ax = axes[idx]
        
        # éš¨æ©Ÿæ¡æ¨£ä»¥é¿å…éåº¦æ“æ“ 
        n_samples = min(2000, len(retain_features), len(forget_features))
        
        retain_idx = np.random.choice(len(retain_features), n_samples, False)
        forget_idx = np.random.choice(len(forget_features), min(n_samples, len(forget_features)), False)
        test_idx = np.random.choice(len(test_features), min(n_samples, len(test_features)), False)
        
        # ç¹ªè£½æ•£é»åœ–
        ax.scatter(retain_features[retain_idx, i], retain_features[retain_idx, j], 
                  alpha=0.5, s=20, color='blue', label='Retain (Member)')
        ax.scatter(forget_features[forget_idx, i], forget_features[forget_idx, j], 
                  alpha=0.5, s=20, color='red', label='Forget (Non-member)')
        ax.scatter(test_features[test_idx, i], test_features[test_idx, j], 
                  alpha=0.3, s=10, color='green', label='Test (Non-member)')
        
        ax.set_xlabel(feature_names[i])
        ax.set_ylabel(feature_names[j])
        ax.set_title(f'{feature_names[i]} vs {feature_names[j]}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… 2Dæ•£é»åœ–å·²ä¿å­˜: {save_path}")
    plt.close()


def plot_pca_visualization(retain_features, forget_features, test_features, save_path=None):
    """
    ä½¿ç”¨ PCA é€²è¡Œé™ç¶­è¦–è¦ºåŒ–
    """
    # åˆä½µæ•¸æ“š
    all_features = np.vstack([retain_features, forget_features, test_features])
    labels = np.concatenate([
        np.ones(len(retain_features)) * 0,      # Retain
        np.ones(len(forget_features)) * 1,      # Forget  
        np.ones(len(test_features)) * 2         # Test
    ])
    
    # PCA é™ç¶­
    pca = PCA(n_components=2)
    features_2d = pca.fit_transform(all_features)
    
    # åˆ†çµ„
    retain_2d = features_2d[labels == 0]
    forget_2d = features_2d[labels == 1] 
    test_2d = features_2d[labels == 2]
    
    # ç¹ªåœ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # PCA æ•£é»åœ–
    ax1.scatter(retain_2d[:, 0], retain_2d[:, 1], alpha=0.6, s=20, 
               color='blue', label=f'Retain (Member) - {len(retain_2d)}')
    ax1.scatter(forget_2d[:, 0], forget_2d[:, 1], alpha=0.6, s=20, 
               color='red', label=f'Forget (Non-member) - {len(forget_2d)}')
    ax1.scatter(test_2d[:, 0], test_2d[:, 1], alpha=0.4, s=10, 
               color='green', label=f'Test (Non-member) - {len(test_2d)}')
    
    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
    ax1.set_title('PCA Visualization of MIA Features')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç‰¹å¾µé‡è¦æ€§
    feature_names = ['confidence', 'entropy', 'margin', 'loss']
    
    pc1_importance = abs(pca.components_[0])
    pc2_importance = abs(pca.components_[1])
    
    x = np.arange(len(feature_names))
    width = 0.35
    
    ax2.bar(x - width/2, pc1_importance, width, label='PC1', alpha=0.8)
    ax2.bar(x + width/2, pc2_importance, width, label='PC2', alpha=0.8)
    
    ax2.set_xlabel('Features')
    ax2.set_ylabel('Absolute Loading')
    ax2.set_title('PCA Feature Importance')
    ax2.set_xticks(x)
    ax2.set_xticklabels(feature_names, rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… PCAè¦–è¦ºåŒ–å·²ä¿å­˜: {save_path}")
    plt.close()
    
    print(f"ğŸ“Š PCA è§£é‡‹æ–¹å·®: PC1={pca.explained_variance_ratio_[0]:.1%}, "
          f"PC2={pca.explained_variance_ratio_[1]:.1%}, "
          f"ç¸½è¨ˆ={pca.explained_variance_ratio_[:2].sum():.1%}")


def plot_tsne_visualization(retain_features, forget_features, test_features, save_path=None):
    """
    ä½¿ç”¨ t-SNE é€²è¡Œéç·šæ€§é™ç¶­è¦–è¦ºåŒ–
    """
    # é™åˆ¶æ¨£æœ¬æ•¸é‡ä»¥åŠ é€Ÿ t-SNE
    max_samples = 3000
    
    retain_subset = retain_features[:min(max_samples, len(retain_features))]
    forget_subset = forget_features[:min(max_samples, len(forget_features))]
    test_subset = test_features[:min(max_samples, len(test_features))]
    
    # åˆä½µæ•¸æ“š
    all_features = np.vstack([retain_subset, forget_subset, test_subset])
    labels = np.concatenate([
        np.ones(len(retain_subset)) * 0,
        np.ones(len(forget_subset)) * 1,
        np.ones(len(test_subset)) * 2
    ])
    
    # t-SNE é™ç¶­
    print("åŸ·è¡Œ t-SNE é™ç¶­ï¼ˆå¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    features_2d = tsne.fit_transform(all_features)
    
    # åˆ†çµ„
    retain_2d = features_2d[labels == 0]
    forget_2d = features_2d[labels == 1]
    test_2d = features_2d[labels == 2]
    
    # ç¹ªåœ–
    plt.figure(figsize=(12, 8))
    
    plt.scatter(retain_2d[:, 0], retain_2d[:, 1], alpha=0.6, s=20, 
               color='blue', label=f'Retain (Member) - {len(retain_2d)}')
    plt.scatter(forget_2d[:, 0], forget_2d[:, 1], alpha=0.6, s=20, 
               color='red', label=f'Forget (Non-member) - {len(forget_2d)}')
    plt.scatter(test_2d[:, 0], test_2d[:, 1], alpha=0.4, s=10, 
               color='green', label=f'Test (Non-member) - {len(test_2d)}')
    
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.title('t-SNE Visualization of MIA Features')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… t-SNEè¦–è¦ºåŒ–å·²ä¿å­˜: {save_path}")
    plt.close()


def create_summary_report(retain_features, forget_features, test_features, output_dir):
    """
    å‰µå»ºæ–‡å­—æ‘˜è¦å ±å‘Š
    """
    feature_names = ['confidence', 'entropy', 'margin', 'loss']
    
    report_path = os.path.join(output_dir, "feature_analysis_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("MIA ç‰¹å¾µåˆ†æå ±å‘Š\n")
        f.write("="*80 + "\n\n")
        
        # åŸºæœ¬çµ±è¨ˆ
        f.write("ğŸ“Š æ•¸æ“šé‡çµ±è¨ˆ:\n")
        f.write(f"ä¿ç•™é›† (æˆå“¡): {len(retain_features):,} æ¨£æœ¬\n")
        f.write(f"éºå¿˜é›† (éæˆå“¡): {len(forget_features):,} æ¨£æœ¬\n")
        f.write(f"æ¸¬è©¦é›† (éæˆå“¡): {len(test_features):,} æ¨£æœ¬\n\n")
        
        # ç‰¹å¾µçµ±è¨ˆ
        f.write("ğŸ“ˆ ç‰¹å¾µçµ±è¨ˆ:\n")
        f.write(f"{'ç‰¹å¾µåç¨±':<12} {'ä¿ç•™å‡å€¼':<10} {'éºå¿˜å‡å€¼':<10} {'æ¸¬è©¦å‡å€¼':<10} {'R-Få·®ç•°':<10} {'é‡ç–Šåº¦':<10}\n")
        f.write("-" * 80 + "\n")
        
        feature_stats = []
        for i, name in enumerate(feature_names):
            retain_vals = retain_features[:, i]
            forget_vals = forget_features[:, i]
            test_vals = test_features[:, i]
            
            retain_mean = retain_vals.mean()
            forget_mean = forget_vals.mean()
            test_mean = test_vals.mean()
            diff = abs(retain_mean - forget_mean)
            overlap = calculate_overlap(retain_vals, forget_vals)
            
            f.write(f"{name:<12} {retain_mean:<10.4f} {forget_mean:<10.4f} "
                   f"{test_mean:<10.4f} {diff:<10.4f} {overlap:<10.3f}\n")
            
            feature_stats.append({
                'name': name,
                'diff': diff,
                'overlap': overlap,
                'retain_mean': retain_mean,
                'forget_mean': forget_mean
            })
        
        # åˆ†æçµè«–
        f.write(f"\nğŸ¯ é—œéµç™¼ç¾:\n")
        
        # æœ€å…·å€åˆ†æ€§çš„ç‰¹å¾µ
        feature_stats.sort(key=lambda x: x['diff'], reverse=True)
        f.write(f"æœ€å…·å€åˆ†æ€§çš„ç‰¹å¾µ:\n")
        for i, stat in enumerate(feature_stats[:3]):
            f.write(f"  {i+1}. {stat['name']}: å·®ç•°={stat['diff']:.4f}\n")
        
        # é‡ç–Šåº¦æœ€é«˜çš„ç‰¹å¾µ
        feature_stats.sort(key=lambda x: x['overlap'], reverse=True)
        f.write(f"\né‡ç–Šåº¦æœ€é«˜çš„ç‰¹å¾µ:\n")
        for i, stat in enumerate(feature_stats[:3]):
            f.write(f"  {i+1}. {stat['name']}: é‡ç–Šåº¦={stat['overlap']:.3f}\n")
        
        # å•é¡Œè¨ºæ–·
        f.write(f"\nğŸš¨ å•é¡Œè¨ºæ–·:\n")
        
        # æª¢æŸ¥éåº¦è‡ªä¿¡
        confidence_high_retain = (retain_features[:, 0] > 0.95).mean()
        confidence_high_forget = (forget_features[:, 0] > 0.95).mean()
        
        if confidence_high_retain > 0.8:
            f.write(f"âš ï¸ ä¿ç•™é›†éåº¦è‡ªä¿¡: {confidence_high_retain:.1%} æ¨£æœ¬ç½®ä¿¡åº¦ >0.95\n")
        
        if confidence_high_forget > 0.5:
            f.write(f"âš ï¸ éºå¿˜é›†ä»ç„¶è‡ªä¿¡: {confidence_high_forget:.1%} æ¨£æœ¬ç½®ä¿¡åº¦ >0.95\n")
            f.write("  â†’ è¡¨æ˜éºå¿˜æ•ˆæœä¸è¶³\n")
        
        # æª¢æŸ¥ç‰¹å¾µè®Šç•°æ€§
        low_variance_features = []
        for i, name in enumerate(feature_names):
            if retain_features[:, i].var() < 0.01:
                low_variance_features.append(name)
        
        if low_variance_features:
            f.write(f"âš ï¸ ä½è®Šç•°æ€§ç‰¹å¾µ: {', '.join(low_variance_features)}\n")
            f.write("  â†’ ç‰¹å¾µç¼ºä¹å€åˆ†èƒ½åŠ›\n")
        
        f.write(f"\nğŸ’¡ å»ºè­°:\n")
        if confidence_high_forget > 0.7:
            f.write("1. å¢åŠ éºå¿˜å¼·åº¦ (æé«˜ä¿®å‰ªæ¯”ä¾‹åˆ°30-50%)\n")
        if len(low_variance_features) > 3:
            f.write("2. å¢åŠ æ›´æ•æ„Ÿçš„ç‰¹å¾µ (å¦‚loss-basedç‰¹å¾µ)\n")
        if all(stat['overlap'] > 0.8 for stat in feature_stats[:4]):
            f.write("3. è€ƒæ…®ä½¿ç”¨æ›´æ¿€é€²çš„éºå¿˜æ–¹æ³•\n")
    
    print(f"âœ… åˆ†æå ±å‘Šå·²ä¿å­˜: {report_path}")


def print_feature_statistics(retain_features, forget_features, test_features, feature_names):
    """æ‰“å°è©³ç´°çš„ç‰¹å¾µçµ±è¨ˆ"""
    print("\n" + "="*80)
    print("ğŸ“Š ç‰¹å¾µçµ±è¨ˆåˆ†æ")
    print("="*80)
    
    feature_stats = []
    
    for i, name in enumerate(feature_names):
        retain_vals = retain_features[:, i]
        forget_vals = forget_features[:, i]
        test_vals = test_features[:, i]
        
        stats = {
            'feature': name,
            'retain_mean': retain_vals.mean(),
            'forget_mean': forget_vals.mean(),
            'test_mean': test_vals.mean(),
            'retain_std': retain_vals.std(),
            'forget_std': forget_vals.std(),
            'test_std': test_vals.std(),
            'retain_forget_diff': abs(retain_vals.mean() - forget_vals.mean()),
            'retain_test_diff': abs(retain_vals.mean() - test_vals.mean()),
            'overlap_rf': calculate_overlap(retain_vals, forget_vals),
            'overlap_rt': calculate_overlap(retain_vals, test_vals)
        }
        feature_stats.append(stats)
    
    # å‰µå»º DataFrame ä¾¿æ–¼æŸ¥çœ‹
    df = pd.DataFrame(feature_stats)
    
    print("ğŸ“ˆ å„ç‰¹å¾µçµ±è¨ˆæ‘˜è¦:")
    print(f"{'ç‰¹å¾µ':<12} {'ä¿ç•™å‡å€¼':<8} {'éºå¿˜å‡å€¼':<8} {'æ¸¬è©¦å‡å€¼':<8} {'R-Få·®ç•°':<8} {'R-Fé‡ç–Š':<8}")
    print("-" * 70)
    
    for _, row in df.iterrows():
        print(f"{row['feature']:<12} {row['retain_mean']:<8.3f} {row['forget_mean']:<8.3f} "
              f"{row['test_mean']:<8.3f} {row['retain_forget_diff']:<8.3f} {row['overlap_rf']:<8.3f}")
    
    # æ‰¾å‡ºæœ€å…·å€åˆ†æ€§çš„ç‰¹å¾µ
    df_sorted = df.sort_values('retain_forget_diff', ascending=False)
    print(f"\nğŸ¯ æœ€å…·å€åˆ†æ€§çš„ç‰¹å¾µ:")
    for i, (_, row) in enumerate(df_sorted.head(3).iterrows()):
        print(f"{i+1}. {row['feature']}: å·®ç•°={row['retain_forget_diff']:.4f}, "
              f"é‡ç–Šåº¦={row['overlap_rf']:.3f}")
    
    print(f"\nâš ï¸  é‡ç–Šåº¦æœ€é«˜çš„ç‰¹å¾µ:")
    df_overlap = df.sort_values('overlap_rf', ascending=False)
    for i, (_, row) in enumerate(df_overlap.head(3).iterrows()):
        print(f"{i+1}. {row['feature']}: é‡ç–Šåº¦={row['overlap_rf']:.3f}, "
              f"å·®ç•°={row['retain_forget_diff']:.4f}")


def analyze_feature_overlap(retain_features, forget_features, test_features):
    """åˆ†æç‰¹å¾µé‡ç–Šçš„åŸå› """
    print("\n" + "="*80)
    print("ğŸ” ç‰¹å¾µé‡ç–ŠåŸå› åˆ†æ")
    print("="*80)
    
    feature_names = ['confidence', 'entropy', 'margin', 'loss']
    
    # 1. æª¢æŸ¥æ¥µå€¼åˆ†å¸ƒ
    print("1ï¸âƒ£ æ¥µå€¼åˆ†å¸ƒæª¢æŸ¥:")
    for i, name in enumerate(feature_names):
        retain_extreme = np.sum((retain_features[:, i] > 0.95) | (retain_features[:, i] < 0.05))
        forget_extreme = np.sum((forget_features[:, i] > 0.95) | (forget_features[:, i] < 0.05))
        
        retain_ratio = retain_extreme / len(retain_features)
        forget_ratio = forget_extreme / len(forget_features)
        
        if retain_ratio > 0.8 or forget_ratio > 0.8:
            print(f"   âš ï¸ {name}: ä¿ç•™é›† {retain_ratio:.1%}, éºå¿˜é›† {forget_ratio:.1%} ç‚ºæ¥µå€¼")
    
    # 2. æª¢æŸ¥æ–¹å·®
    print("\n2ï¸âƒ£ ç‰¹å¾µæ–¹å·®åˆ†æ:")
    for i, name in enumerate(feature_names):
        retain_var = retain_features[:, i].var()
        forget_var = forget_features[:, i].var()
        
        if retain_var < 0.01 and forget_var < 0.01:
            print(f"   âš ï¸ {name}: æ–¹å·®éå° (ä¿ç•™:{retain_var:.4f}, éºå¿˜:{forget_var:.4f})")
    
    # 3. æª¢æŸ¥æ˜¯å¦æ‰€æœ‰æ¨£æœ¬éƒ½éåº¦è‡ªä¿¡
    confidence_high = (retain_features[:, 0] > 0.95).mean()
    entropy_low = (retain_features[:, 1] < 0.1).mean()
    
    print(f"\n3ï¸âƒ£ æ¨¡å‹è‡ªä¿¡åº¦æª¢æŸ¥:")
    print(f"   ä¿ç•™é›†é«˜ç½®ä¿¡åº¦ (>0.95): {confidence_high:.1%}")
    print(f"   ä¿ç•™é›†ä½ç†µ (<0.1): {entropy_low:.1%}")
    
    if confidence_high > 0.8:
        print("   ğŸš¨ æ¨¡å‹å°ä¿ç•™é›†éåº¦è‡ªä¿¡ï¼Œå¯èƒ½å°è‡´ç‰¹å¾µå€åˆ†åº¦ä¸è¶³")
    
    forget_confidence_high = (forget_features[:, 0] > 0.95).mean()
    forget_entropy_low = (forget_features[:, 1] < 0.1).mean()
    
    print(f"   éºå¿˜é›†é«˜ç½®ä¿¡åº¦ (>0.95): {forget_confidence_high:.1%}")
    print(f"   éºå¿˜é›†ä½ç†µ (<0.1): {forget_entropy_low:.1%}")
    
    if forget_confidence_high > 0.5:
        print("   ğŸš¨ æ¨¡å‹å°éºå¿˜é›†ä»ç„¶éåº¦è‡ªä¿¡ï¼Œéºå¿˜æ•ˆæœä¸è¶³")


# ä¸»è¦èª¿ç”¨å‡½æ•¸
def visualize_mia_features(retain_features, forget_features, test_features, 
                          output_dir="./mia_analysis"):
    """
    å®Œæ•´çš„ MIA ç‰¹å¾µè¦–è¦ºåŒ–åˆ†æ
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # print("ğŸ¨ é–‹å§‹ MIA ç‰¹å¾µè¦–è¦ºåŒ–åˆ†æ...")
    print(f"[Debug] retain={retain_features.shape}, forget={forget_features.shape}, test={test_features.shape}")
    
    # 1. ç‰¹å¾µåˆ†å¸ƒåœ–
    print("\n1ï¸âƒ£ ç¹ªè£½ç‰¹å¾µåˆ†å¸ƒåœ–...")
    plot_feature_distributions(
        retain_features, forget_features, test_features,
        save_path=os.path.join(output_dir, "feature_distributions.png")
    )
    
    # 2. 2D æ•£é»åœ–
    print("\n2ï¸âƒ£ ç¹ªè£½2Dç‰¹å¾µæ•£é»åœ–...")
    plot_2d_feature_scatter(
        retain_features, forget_features, test_features,
        save_path=os.path.join(output_dir, "feature_scatter_2d.png")
    )
    
    # 3. PCA è¦–è¦ºåŒ–
    print("\n3ï¸âƒ£ PCA é™ç¶­è¦–è¦ºåŒ–...")
    plot_pca_visualization(
        retain_features, forget_features, test_features,
        save_path=os.path.join(output_dir, "pca_visualization.png")
    )
    
    # 4. t-SNE è¦–è¦ºåŒ–
    print("\n4ï¸âƒ£ t-SNE éç·šæ€§é™ç¶­è¦–è¦ºåŒ–...")
    plot_tsne_visualization(
        retain_features, forget_features, test_features,
        save_path=os.path.join(output_dir, "tsne_visualization.png")
    )
    
    # 5. é‡ç–ŠåŸå› åˆ†æ
    print("\n5ï¸âƒ£ åˆ†æç‰¹å¾µé‡ç–ŠåŸå› ...")
    analyze_feature_overlap(retain_features, forget_features, test_features)
    
    # 6. å‰µå»ºæ–‡å­—å ±å‘Š
    print("\n6ï¸âƒ£ ç”Ÿæˆåˆ†æå ±å‘Š...")
    create_summary_report(retain_features, forget_features, test_features, output_dir)
    
    print(f"\nâœ… è¦–è¦ºåŒ–å®Œæˆï¼æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - feature_distributions.png: ç‰¹å¾µåˆ†å¸ƒå°æ¯”")
    print(f"   - feature_scatter_2d.png: 2Dç‰¹å¾µæ•£é»åœ–")
    print(f"   - pca_visualization.png: PCAé™ç¶­è¦–è¦ºåŒ–")
    print(f"   - tsne_visualization.png: t-SNEéç·šæ€§é™ç¶­")
    print(f"   - feature_analysis_report.txt: è©³ç´°åˆ†æå ±å‘Š")